% !TEX root = ../root.tex

\chapter{Discussion}
% Results and discussion, 15-25 pages

For a detailed discussion of each paper, we refer to the papers themselves. Here we will discuss the common themes, the connections between them, and what the results of this thesis mean for future work.

\section{Designing robots that can avoid failure}

As demonstrated in~\cite{heim2018shaping}, robustness to failures is critical to allowing learning control, and appropriate system design can greatly influence the inherent system robustness. Our work in~\cite{heim2019beyond} formalizes what a system should strive for: robust natural dynamics. \par
Although an example application of computational design is given in~\cite{heim2019beyond}, it is limited to optimizing the parameters of a low-level control for a relatively low-dimensional system. To truly make use of these concepts for computational design (both of robot hardware and low-level controls) will require further progress in scaling these tools to compute viable sets in state-action space. \par
The work in~\cite{heim2019learnable} begins to address this: although the main contribution of the paper is to enable safe learning directly on a robot, it can also be used in simulation to learn approximations of viable sets, and scales better than the brute-force computation used in~\cite{heim2019beyond}.
Nonetheless, further tools to allow computational tractability are needed to make these concepts useful for higher-dimensional systems. Exploiting structure in well-understood dynamical systems, such as the rigid-body dynamics commonly seen in robots, is a promising path. How this structure relates to the set-valued dynamics view of viability is an open question. \par
In parallel to developing computational tools, we also see promise in the knowledge gleaned from reduced order models. Indeed, armed with the knowledge that robustness to noisy action spaces will make controller design easier, a robot designer can already make deliberate design decisions guided by intuition.
 % \par


% To be able to learn, an agent must be able to sample information that is useful for the learning process. In most cases, this means to sample a gradient in the reward function.
% However, being able to sample gradients consistently is often a challenge, especially in robotics. This is often addressed with a focus on the algorithmic side: common solutions include reward shaping, designing better representations, and curriculum learning. An often overlooked approach is by designing the system itself. While this approach has already been justified both in practice~\cite{tedrake2005learning} and with some theoretical results~\cite{randlov2000shaping}, to the best of our knowledge the results are not extensive and have not been further elaborated. \par
% In~\cite{heim2018shaping}, we show empirically that avoiding failures is typically synonymous with avoiding uninformative samples. In~\cite{heim2019beyond}, we show formalize 
% % My work shows the i role that the system design itself plays an important role in this, both in practice~\cite{heim2018shaping} and in theory~\cite{heim2019beyond}.
% In particular, I explore and formalize the case of robustness against failures. Not only do failures potentially harm the robot, the world, and require time-consuming resets, they often provides only marginally useful information. Indeed, drastic failures and borderline failures often return the same reward. In order to leverage learning control, new robot designs should directly consider the robustness of the system, in addition to performance, manufacturing and maintenance costs, etc. \par
% This is particularly the case for general-purpose robots that do not have pre-specified tasks. These types of robots are becoming more common as robots become more common-place both in the workplace and in society as a whole.
% Examples include the Baxter robot, which are specifically designed so their task can be quickly redefined by novice users, or the newly launched (at the time of writing) Spot quadruped by Boston Dynamics (see \url{https://www.bostondynamics.com/spot}).
% In these cases, it can still be straightforward to define sets of failure states, which are often the same regardless of the task. These failure sets can therefore be defined at the early design stage, and the robot can be designed with to be robust to them. \par

% For example, legs with some built-in mechanical stiffness in the form of springs are known to have self-stabilizing properties SPROWITZ. Although mechanical springs have the drawback of being difficult to tune, this may be an acceptable trade-off. \par

% In a similar manner, when designing the low-level controller for an impedance-controlled robot, it may be desirable to begin with low gains. This would presumably improve the overall robustness of a high-level control policy, at the cost of lowering the performance of the optimal policy. In a similar manner to~\textcite{heim2018shaping}, the gains can then be slowly tuned to more aggressive values as the learning progressed. \par

%%%%
% - repeating theme: robustness of sampling a reward improves our ability to learn. 
% - this is regardless of the search space: it can be parameters of a controller or directly on the action space
% - We should therefore design robots to be able to sample rewards better. The common approach is curriculum learning, we show however that shaping the natural dynamics can play an important role. This was already shown by TEDRAKE et al., and RANDLOV
% - It is, however, difficult to do that in practice. Armed with this knowledge, we can use it as an intuitive guideline in 
% - To obtain more rigorous principles that can be scaled up to practical problems is an open problem. Here are some IDEAS

\section{Designing robots that can fail}
The work of this thesis also emphasizes the need for robots to be designed such that occasional failures are not critical. The most important reason for this is that guaranteeing safety requires perfect models, which, of course, do not exist~\cite{box1976science}. The computational tractability of sufficiently accurate models is, as discussed above, still a major challenge. It is then only reasonable to begin with simpler, less accurate but more tractable models, before transferring directly to the real system. To use these less accurate results in practice, we can take two approaches. \par
One approach is to use provably under-approximated viable sets, which will retain guarantees of safe operation. However, this approach is limited for two reasons. First, obtain such under-approximations also suffers from poor scaling properties. Second, these under-approximations often result in over-conservative behaviors. \par
A different approach, which is briefly mentioned in one of the examples in~\cite{heim2019learnable}, is to use data from actual operation of the robot to refine an initial simulation-driven approximation of the viable set. This approach, which we find promising, is contingent on robots being allowed to occasionally cross the boundary of failure. In practice, this means a robot should be sturdy enough to survive failures, but also that a failure does not cause unacceptable damage to the user or environment, and it is possible to reset the robot quickly and easily. Just as in curriculum learning, it may be possible to gather at least a portion of these data samples in a controlled environment, a practice room so to speak, where failures are tolerable. \par

A second reason why robots should be designed to allow failures is illustrated in~\cite{heim2018unviable}. The core concept is that systems often have large sets of unviable states: states which have not failed yet, but are doomed to fail within finite time. While marching towards failure, the learning agent can still explore and collect informative samples that are useful for the learning process - if it can survive the failure. For this to be useful, a designer should observe two requirements: first, the reward function should be designed such that there is relevant information in these unviable states. Second, the system should be designed such that failures do not happen abruptly, but that there is as much time as possible before failure actually happens. Fortunately, this requirement often goes hand in hand with improving robustness, as discussed above. For example, starting with soft gains on an impedance controller will allow a robot to stumble and fall more slowly than a controller that is aggressively tuned for performance.

% Second reason, \textcite{heim2018unviable} show the counter-intuitive result that it can be more reliable to initialize a learning agent in unviable states. This is because many systems often have large sets of unviable states: states which have not failed yet, but are doomed to fail within finite time.
% If a robot can be reset after failure, this time can still be used constructively to sample information relevant to the learning task. This poses two requirements: first, the reward function should be designed such that there is relevant information in these unviable states. Second, the system should be designed such that failures do not happen abruptly, but that there is as much time as possible before failure actually happens. Fortunately, this requirement often goes hand in hand with improving robustness, as discussed above. For example, starting with soft gains on an impedance controller will allow a robot to stumble and fall more slowly than a controller that is aggressively tuned for performance.

% - when designing robots, this stresses the importance of being mechanically sturdy, such that it is possible to visit failure states without permanent damage. This property can be seen in many production-level machines.
% - The main reason why failure states pose a problem to learning, is that in most cases, any failure returns no reward and therefore provides no learning gradient. This is why designing a robot that can fail with grace can greatly help learn.

\section{Leveraging dynamics models} \label{chap:discussion}
I have focused on model-free methods, for a very deliberate reason: I believe that model-based optimal control is a fantastically powerful tool, and it is too tempting to try to reformulate any problem as an optimization problem.
At the same time, I believe that robustness is of paramount importance and has been overshadowed by the optimality perspective of dynamics.
Taking a completely (or nearly) model-free approach forces us to deal directly with robustness, as we have no assumed structure to exploit.
It has strongly motivated the development of new, mathematically simple tools described in~\cite{heim2019beyond}, and the clean simplicity for safe learning in~\cite{heim2019learnable}.
% For example, contemporary work on safe learning based on viability make use of relatively sophisticated mathematical tools such as differential inclusions AUBIN, barrier functions EGERSTEDT-AMES, or solving Hamilton-Jacobi-Issacs equations TOMLIN.
Now that the fundamental mathematical objects have been established, the time is ripe to turn to model-based tools. Leveraging this structure will be an essential key to scaling up these concepts to useful models in higher dimensions. \par
Several computational tools are being developed to compute invariant sets in state-space. For example, sums-of-squares polynomials can be cast as semi-definite programs (SOS programming), and have been used to compute robust funnels~\cite{majumdar2013robust} and robust regions of attraction~\cite{valmorbida2014roa_invariants}. Although these tools are typically used for analyzing convergence in the Lyapunov sense, it is reasonable to expect the same tools to work well for viable sets, which are also invariant sets. \par
While these tools are computationally rather demanding, they have mostly been used for analysis. However, recent developments are pushing their utility for control, for example by combining SOS programming with reachability analysis in a hierarchical framework~\cite{singh2018robust}, or combining it with trajectory optimization~\cite{manchester2019robust}. Similar combinations with a view of viability in state-action space have, in my opinion, much potential and offer exciting opportunities. \par
Directly learning a model of the dynamics while also learning a model of the safety measure (as in~\cite{heim2019learnable}) is, in my opinion, one of the most promising yet straightforward next steps. In particular, each of these learning processes can benefit from the other in terms of sample efficiency: on the one hand, it is desirable to focus sampling in viable regions when gathering samples for learning dynamics. On the other hand, a model of the dynamics can be used to bootstrap learning the safety measure. Furthermore, predictions can help active sampling by 'checking ahead' to compare the safety of a state-action pair with the safety of the predicted next state. How to trade off between uncertainty of the prediction compared to the model of the safety measure is something I hope to look into soon.