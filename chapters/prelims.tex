% !TEX root = ../root.tex

\chapter{Preliminaries} \label{chap:prelims}
This chapter introduces relevant background information to help readers from potentially different backgrounds to quickly gain enough knowledge to understand the context, relevance, and importance of the contributions of each paper, in particular as related to dynamic legged locomotion. Covered are the basics viability theory, legged locomotion, and reinforcement learning.

\section{Viability and Backreachability}

The core of my work is based on viability theory, first pioneered by Jean-Pierre Aubin~\cite{aubin2011viability}. The development of viability theory is largely motivated by observations of dynamical systems in nature and society, whose behavior somehow avoids chaos or pure randomness, yet never seem to settle at a resting state, an equilibrium.
Some examples are Darwinian evolution, economics, or politics of the state. There is no terminal equilibrium state for these systems, or at least, none that we can identify or foresee.
The classical mathematical tools based on convergence to such an equilibrium state are therefore ill-suited to describe them, although they can be manipulated for the purpose: for example, by assuming a quickly time-varying equilibrium state, which the system chases but is never able to reach. \par
Viability theory provides a more appropriate and direct description. First, sets of failure states are defined, that the system must be able to avoid. We will think of this failure set as a set of absorbing states. From this naturally emerges the \emph{viability kernel}, the maximal set of states from which there exist control inputs that keep the system inside the viability kernel for all time. In simpler terms, if the system ever leaves the viability kernel, it can no longer return inside of it and is doomed to enter the failure set within finite time. \par
Compared to mathematical objects based on the notion of convergence, such as regions of attraction~\cite[(section 6.4)]{strogatz2018nonlinear} or contractions~\cite{bazzi2018stability}, conditions for viability are weaker (they provide no convergence properties) but more general. \par
Two other related sets which depart from the notion of convergence are \emph{backreachable sets}~\cite{bansal2017hamilton} and \emph{controllable sets}~\cite{zaytsev2018boundaries}. For backreachable sets, we start by defining a target set.
The backreachable set is the set of states from which there exist control inputs that guide the system into the target set within finite time.
Since being able to reach this target set directly implies being able to avoid the failure set, the back-reachable set is always a subset of the viability kernel.
Controllable sets require that every point in the set is reachable from anywhere in the set. In other words, the controllable set must be a backreachable set for all subsets of the controllable set itself. Controllability, sometimes referred to as ergodicity in the context of Markov decision processes~\cite{moldovan2012safemdp}, is a more general statement than backreachability in that it does not require the definition of a target state, but also a stronger statement: not all backreachable sets are controllable sets. By the same quality of backreachable sets, controllable sets are also subsets of the viability kernel. \par
Viability theory, therefore, allows us to make very general statements of a dynamical system's behavior, without making many assumptions or conditions. This generality comes at a price: computing viable sets is typically computationally expensive. For the types of dynamics we are interested in, we will typically resort to gridding and brute force. In this thesis, I will use the concepts of viability theory, but will not concern ourselves with algorithms that scale to higher dimensions. For the interested reader, I recommend starting with~\cite{liniger2017real} for a clear and practical application,~\cite{deffuant2007approximating} for an example of sample-driven approximations, and~\cite{bansal2017hamilton} for an overview of tools for the computation of the related backreachable sets.

\subsection{Why use viability for legged locomotion?}

The study of legged locomotion has come a long way by thinking of gaits as stable limit-cycles: periodic orbits through state space towards which nearby orbits converge. This view allows us to explain various observations with a well developed mathematical language: nonlinear dynamics and bifurcations~\cite{strogatz2018nonlinear}. \par
For example, the remarkable stability of running dynamics~\cite{daley2006running} can be understood by studying basins of attraction of a specific gait~\cite{merker2015stable,cnops2015basin}. The choice and transition between different gaits depending on locomotion speed or morphology can be understood via bifurcation studies~\cite{owaki2013simple,aoi2013stability,gan2018all}. \par

However, these tools hinge on the existence of a limit-cycle, and therefore are ill-suited for understanding unsteady or erratic motion, which is also commonly observed~\cite{wheatley2015escape,moore2017unpredictability}.
Even when it is reasonable to assume the existence of a limit-cycle, for example when walking or running on a treadmill at a steady speed, it is often difficult to fit recorded time-series data to these assumptions~\cite{bruijn2013assessing, maus2015constructing}. To gain further insight into locomotion, we need tools that can reason about stability in the sense of avoiding falls instead of in the sense of convergence~\cite{Birn-Jeffery3786}. \par

Viability theory provides a natural description for this type of stability. Furthermore, viability makes no assumptions on the control policy or the task at hand. This makes it appropriate for analyzing morphology separately from the control policy, as in~\cite{heim2019beyond}. \par

In the field of legged robotics and control, there have also been developments which move away from the perspective of strict convergence.~\textcite{byl2009metastable} relax the requirement of convergence with the probabilistic concept of \emph{metastability}, and compute the mean first passage of time for several simple models of legged locomotion. This metric takes into account the probability of failure from any sources of uncertainty, and can effectively serve as a metric of task-level robustness.
However, the structure of how dynamics relate to failures is not exposed. Furthermore, changing any source of uncertainty, such as the distribution of disturbances, requires the mean first-passage of time to be recomputed from scratch. \par

Capture points and capture regions~\cite{koolen2012capturability} build on the ideas of backreachability to find effective yet simple foot placements that can come to a standstill. In this context, a \emph{capture point} represents a foot placement that takes the system to a \emph{captured state}: standing, static equilibrium. An \emph{$n$-step capture region} is the set of foot placements that take the system to a state, from which the captured state can be reached within $n-1$ steps.
The core idea is that it is not necessary for a controller to always seek convergence, as long as the system stays within a region from which it can converge within a finite number of steps.
Capture regions are particularly useful in legged locomotion since they are typically easier to compute than viability kernels, and usually as large or nearly so~\cite{koolen2012capturability,zaytsev2018boundaries}. The core results of this thesis, namely~\cite{heim2019beyond,heim2019learnable}, can be reformulated to use capture regions instead of viable sets, without much loss of precision. I find, nonetheless, the more exact language of viability theory to be useful in formalizing the mathematical objects developed in this thesis. \par

\section{Dynamic legged locomotion}
While most results in my publications are valid for arbitrary dynamical systems, the motivation is grounded in legged locomotion, in particular running. This field offers abundant natural inspiration, including ourselves. Legged systems also encapsulate many interesting challenges in dynamics: the dynamics of legged systems are non-smooth, due to impacts at foot touchdown. They are hybrid, as the governing equations of motion switch abruptly every time a foot touches down or lifts off. They are underactuated, due to the floating base. And of course, they are typically highly nonlinear. While passively stable legged systems exist, for most systems of interest to us, they are passively unstable and require feedback control.
In addition, the community of dynamic legged locomotion is vibrant and friendly. In this section, I will give a brief introduction to the field.

\subsection{Reduced order models of legged locomotion}

Reduced order models allow researchers to cope with the complexity of legged locomotion dynamics. \par
In this thesis, I will focus on the spring-loaded inverted pendulum (SLIP) model for running. This model originated in the biomechanics community to describe center-of-mass movement of running in humans~\cite{blickhan1989spring}. Comprising of a point-mass to represent the body and a massless spring to represent the leg, the model is parsimonious in its parameters, making it relatively easy to fit to data.
Several studies have then fit it to various animals of different sizes and with different numbers of legs~\cite{blickhan1993similarity,daley2006running,jindrich2002dynamic}. A two-legged extension also accurately predicts ground-reaction forces of both running and walking in humans~\cite{geyer2006compliant}. These studies of spring-mass models convincingly show the presence of compliance in natural running motion. Compliance is also often reproduced in biomimetic robots~\cite{sprowitz2013cheetah,buchli2006resonance,hubicki2016atrias,hutter2016anymal,kenneally2016design,ramos2018facilitating} in one form or another. \par
One of the benefits of these models is that it gives a good sense of the system's \emph{natural dynamics}, or how the system naturally `wants' to move.
It is no surprise that, in parallel to the development of this model in the biomechanics community, Raibert developed his famous hopping robots using very similar models and intuition~\cite[see Figure 2.5]{raibert1986legged}.
There is a consensus that running systems should feature dynamics with compliance, and a good control scheme will \emph{exploit the natural dynamics} of the system.
This behavior is also observed in biology:~\textcite{daley2006running} used this model to explain the open-loop robustness to height perturbations observed in running birds. \par
However, since this model is energy-conservative and neglects most degrees of freedom, it is often insufficient to describe many observations of interest. For example, extensions that include actuation have been used to draw conclusions on control priorities~\cite{Birn-Jeffery3786,blum2014swing}. In another example,~\textcite{maus2015constructing} use a data-driven approach to suggest various extended state and control laws for the SLIP, based on experimental observations of humans. % Perhaps discuss this more \par
Reduced order models are also often used for mathematical analysis, as their low dimensionality allows for thorough investigation of the dynamics and parameters.
% For example,~\textcite{kuo2002energetics} analyzed `the simplest walking model', which allowed him to make statements on the energetic benefits of toe-off using simple, first-principles calculations.
In my own master thesis work~\cite{heim2016designing}, I derive the explicit equations of motion (\eom) of a running model with a tail. This is made possible by approximating the tail as a flywheel, and allows insight on scaling effects of different parameters of the tail, which could then be tested directly in hardware. \par
Reduced order models also allow numerical analysis by brute force, due to their low dimensionality. The basins of attraction for many such models have been studied numerically~\cite{schwab2001basin,obayashi2016formation,cnops2015basin,rummel2008stable}, and extended in some cases to bifurcation analysis~\cite{aoi2006bifurcation,merker2015stable,gan2018all}. By making use of the \poincare section, the stability of a limit-cycle can be numerically ascertained by Floquet analysis~\cite{remy2011matlab}.
Reduced order models are also used by~\textcite{byl2009metastable}, since computing the mean first-passage of time requires a very large number of simulations and would be prohibitive for more sophisticated models. \par
These simple models have also been used to study various control concepts~\cite{piovan2013two,cnops2015basin,piovan2015reachability}. In one of the most insightful examples, ~\textcite{wu20133} show an open-loop trajectory for the swing-leg that achieves deadbeat control: any ground-height perturbation can be completely rejected in a single step. \par 
Leaving the world of spring-mass models of running, one of the most successful reduced order models for legged locomotion is the linear inverted pendulum model, commonly used for the zero-moment point algorithm~\cite{kajita2001LIP,kajita2003ZMP}. Using the same and similar models,~\textcite{koolen2012capturability} compute $n$-step capture regions: regions of foot placement for which the system can come to a complete standstill within $n$ steps. Both these approaches can be applied by making use of a hierarchical controller: these simple models are used by a high-level planner. The output of this planner is then tracked by the low-level controller using a full-order model.

\subsection{Heuristic Control for legged locomotion}


% Dynamics of legged locomotion include many interesting properties that make them particularly challenging to control.  \par

A recurrent theme in legged locomotion is to design controllers that can exploit the natural dynamics of the system. Indeed, despite complicated dynamics, it is possible to achieve locomotion with very simple approaches that exploit these natural dynamics.
Perhaps the most extreme examples are those inspired by passive dynamic walkers~\cite{mcgeer1990passive}. These robots feature minimal sensing and actuation, and are typically limited to flat ground~\cite{bhounsule2012design,wisse2006design}. They make up for their lack in maneuverability with extreme efficiency: the controller mostly injects small amounts of energy and then allows the natural dynamics to take their course.~\textcite{tedrake2005learning} saw in natural dynamics more than just the opportunity for efficient motion: they use a biped robot based on the passive dynamic walkers, and show it can efficiently and reliably learn and adapt to changing ground surfaces. One of the key contributions of this thesis is to formalize this insight and show how the intrinsic robustness of a system's natural dynamics helps to learn control. \par
Many of the earlier successful legged robots relied on exploiting natural dynamics by using simple controllers in the form of clocks and oscillators~\cite{sprowitz2013towards,buchli2006resonance,altendorfer2001rhex,owaki2013simple}: motor positions are servoed along predetermined, periodic trajectories to the timing/phase of a clock/oscillator. All of these robots incorporate compliance in the form of mechanical springs. Once tuned to the natural dynamics, the clock and oscillator controllers generate very stable and dynamic gaits. With some feedback, the oscillators also adapt to the natural dynamics, resulting in different gaits depending on the morphology~\cite{owaki2013simple} or driving frequency~\cite{owaki2013simple,owaki2017quadruped}. % OWAKI oscillators with some feedback show adaptation to different natural dynamics
However, these approaches are often limited in versatility and typically rely on a lot of intuition and trial-and-error to design. They are also more challenging to tune for inherently unstable systems such as bipeds. \par
Another approach is to use a hierarchical control scheme, in which the low-level controller causes the system to behave like a reduced order model, such as the SLIP model discussed above.
If this low-level controller is accurate enough, the plethora of control laws studied for the SLIP model can be directly leveraged. To this end, there have been many efforts to design such a low-level controller~\cite{hutter2010slip,poulakakis2009spring,wensing2013high}. Unlike for the linear inverted pendulum model, however, it is not trivial to map between the high-level representation of a SLIP model and low-level representation of an actual robot, and this approach has only seldomly been applied in practice~\cite{martin2017experimental}.
Furthermore, these high-level controllers \emph{exploit the natural dynamics of the reduced order model}, but not necessarily those of the system. It is entirely possible for the actual natural dynamics of the system to be negated by the low-level controllers.
For these reasons, I believe reduced order models such as the SLIP are excellent \emph{descriptive} models but poor \emph{prescriptive} models: they allow a deep understanding, but make for poor control targets. \par
Virtual model control~\cite{pratt2001virtual,renjewski2015exciting} provides a slightly more relaxed approach: the low-level controller simply commands torques to mimic a spring-damper between two arbitrary points, usually between the hip and the foot. This approach doesn't force the system's dynamics onto the submanifold of the SLIP model, but retains the complaint behavior. It is also easy to implement, requiring only knowledge of the kinematics. However, this approach does not capture the impedance of the system, and it can be difficult to stabilize the robot trunk in more aggressive motion.

\subsection{Optimal Control for legged locomotion}
Optimal control offers a more explicit approach to consider the natural dynamics in the controller.
The setting is to find a policy $u_k = \pi(x_k)$  which minimizes the total cost-to-go function $J$ given a dynamical system $x_{k+1} = f(x_k, u_k)$:
\begin{align*}
\text{find } & u_k = \pi(x_k) \\
\text{such that } & J^*(x_0) = \min_u \left[g_N(x_N) + \sum_{k=0}^{N-1}g(x_k, u_k)\right] \\
\text{subject to } & x_{k+1} = f(x_k, u_k)
\end{align*}
where $J^*$ is the optimal cost accumulated between any state $x_0$ and a terminal state $x_N$, $g_N$ is the final cost of being in $x_N$, and $g(x_k, u_k)$ is the cost incurred for applying the control input $u_k$ from state $x_k$.
This formulation explicitly takes the dynamics of the system into consideration, since the solution must be consistent with the dynamics $f(x_k, u_k)$.
One of the core insights in optimal control is the \emph{principle of optimality}: the fact that any subtrajectory of an optimal trajectory is itself optimal for its starting and ending states.
Therefore, the optimal control problem can be split into several small steps, and each of these solved individually.
This principle is leveraged in the Bellman equation (or its continuous-time equivalent, the Hamilton-Jacobi-Bellman equation):
\begin{equation}\label{eq:bellman}
J^*(x_k) = \min_u\left[g(x_k,u_k) + J^*(f(x_k, u_k)) \right].
\end{equation}
If this function is known, finding an optimal controller is reduced to a one-step lookahead optimization.
To find $J^*$, we can simply use the Bellman equation as an update rule iteratively, which leads to the \emph{dynamic programming algorithm}. A typical iteration will start at the final state $x_N$, apply these updates by backing out from the final state. Once $J^*$ has converged, a control policy that greedily follows eq. \ref{eq:bellman} will be optimal from any state.
The main caveat with this approach is that it does not scale well with dimensionality. \par
Perhaps the most popular optimal control approach used in legged locomotion today is trajectory optimization: in this setting, we give up on global optimality and instead seek trajectories that are only locally optimal.
For the interested reader, I recommend~\cite{kelly2017introduction}. Without going into detail, there are two main families of trajectory optimization: shooting methods and direct transcription methods.
\par
Shooting methods iterate making a forward pass, using an initial guess to simulate a trajectory, and then a backward pass to compute updates to the control inputs used.
A popular and fast shooting method is differential dynamic programming~\cite{tassa2011theory}, in which each backward pass updates the control trajectory with a Bellman update using a local, quadratic approximation of the cost-to-go function. This allows the updates to be backed out of the final state, in a conceptually similar way as the dynamic programming algorithm.
Other shooting methods will typically pass the entire trajectory to a general nonlinear programming (NLP) solver, which optimizes over the entire trajectory at once~\cite{kelly2017introduction}. This approach tends to struggle with problems that require complex controls and constraint satisfaction\footnote{I will not attempt a rigorous definition of what more or less 'complex' controls are. Suffice it to say that legged locomotion typically requires controls which are more complex than orbital dynamics of satellites.}, such as legged locomotion. Nonetheless, it has been used successfully for offline design and analysis~\cite{mombaur_2009,remy2011matlab}. \par
Transcription methods break up the problem completely and allow an NLP solver to optimize not only over the control inputs at each time step, but also the states. Dynamic consistency is then enforced by transcribing the dynamics into constraints the solver must satisfy~\cite{kelly2017introduction}. This formulation also allows constraints to be directly encoded in the optimization problem, which is not straight-forward for shooting methods. \par
A significant challenge for all these methods is the presence of contacts in legged locomotion. Making or breaking a contact induces a non-smooth jump in the cost, which solvers struggle with.
While some formulations can directly reason about contacts~\cite{mordatch2012discovery,dai2014whole,winkler2018gait}, they tend to be slow. A common approach is to split the problem into parts that can be solved separately in a hierarchical control scheme. Typically, these will include generating a center of mass (and sometimes momentum) trajectory~\cite{dai2014whole,buchli2009compliant}, a sequence of footstep positions and timings~\cite{buchli2009compliant,deits2014footstep}, and a whole-body controller to execute the first two~\cite{sentis2007synthesis,buchli2009compliant}. Much of the current research aims at improving the robustness~\cite{,manchester2019robust,yeganegi2019robust}, speed~\cite{ponton2018ontime,lin2019efficient}, or flexibility~\cite{boussema2019impulse} of these individual problems.

\subsection{Hardware Design for legged locomotion}

Just as we strive for controllers that can exploit a system's natural dynamics, we strive for hardware designs that feature beneficial natural dynamics.
It has long been considered important to design legged systems around the concept of compliance.
We mentioned above several designs~\cite{sprowitz2013towards,buchli2006resonance,altendorfer2001rhex,owaki2013simple} which use highly-geared motors and position control; the output of these `stiff' actuators then drive relatively soft springs, which provide the desired compliance. The soft springs are typically placed distally, and therefore minimize unsprung mass and protect the motors from harsh impacts. Additionally, simple controllers can achieve remarkable stability while running on cheap hardware with low update rates (on the order of 100 Hz). However, the complexity is traded off into the hardware design, and tuning hardware is usually much more time-consuming and expensive than tuning software. Furthermore, once built, it is usually difficult to change parameters without disassembling the robot. Since different behaviors often require very different natural dynamics, this approach can severely limit a robot's versatility. \par
A second approach is to incorporate the compliance directly in the actuators. Indeed, Raibert's first hoppers used pneumatic actuators, which act as air-springs in addition to injecting energy~\cite{raibert1986legged}. They require, however, cumbersome pumps that are typically heavy and inefficient.
A popular solution are series-elastic actuators (SEA)~\cite{pratt1995series}. In this case, the output of highly-geared motors is coupled in series with a relatively stiff spring. By measuring the position of both ends of the spring, the force output at the end-effector can be directly calculated. This allows force/torque control at the joints, and compliance of relatively arbitrary form can be generated, within the limitations of the motor constraints and kinematics. Several of the most successful robots of today are built with this type of actuation, such as ANYMAL~\cite{hutter2016anymal} or ATRIAS~\cite{hubicki2016atrias}. The main drawbacks are added complexity for manufacturing and low-level control.
\par
A third approach eschews any mechanical springs in favor of generating compliance purely through actuation and control. Low gear-ratios are required to minimize reflected inertia and achieve transparency while still maintaining high peak torque outputs~\cite{Seok2012proprioceptive}.
Fortunately, the widespread success of quadcopters has dramatically reduced the cost of off-the-shelf outrunner motors that satisfy these requirements. Together with field-oriented control, sometimes called vector control, these motors provide a cheap solution at smaller scales. Many small and medium-sized robots capable of highly dynamic motion and direct torque-control are emerging, such as the MIT mini-cheetah~\cite{katz2019mini}, the Minitaur~\cite{kenneally2016design} and MPI-IS's own Solo~\cite{grimminger2019open}. Since mass scales roughly cubically\footnote{Mass will scale cubically to length if we assume isometric scaling and constant, uniform density. While robot designs break these assumptions, this serves as a good rule of thumb.}, these smaller-scale robots are mechanically sturdy~\cite{biewener2005biomechanical} and can operate at torques that are much safer to handle. These robots are excellent experimental platforms, especially for learning control, since failures may be more common. \par

% Regardless of the approach taken, hardware design has remained more an art than a science, with experience and skill of the designer playing a crucial role. An attractive alternative is \emph{co-design}, or allowing an algorithm to simultaneously design both the morphology and controller of the robot. This idea was first pioneered by~\textcite{sims1994evolving}, who used evolutionary algorithms to search over the parameter space of both morphology and controllers simultaneously. More recent work typically makes use of trajectory optimization

% Design of legged robots has come a long way in the last decade. The overall trend is to enable torque control at the joints.

% Scale: we want small robots

% Types of actuation: we want torque-control. This led to hydraulic ATLAS and HyQ, then series-elastic actuation ANYMAL, Cassie. Inspired by the haptics community, SANGBAE proprioceptive motors. Low-gear ratio allows high transparency. Commercial success of quadcopters has lowered the cost of small and mid-sized outrunner motors, allowing a new generation of small and cheap robots with torque control at the joints CITATIONS.

% This new generation of small and cheap robots the ones that most interest us. As research platforms they will allow experimentation on hardware, including model-free learning. The small size lends an inherent sturdiness to failures CITATION. The cheap production allows them to be repaired quickly and easily. 
\section{Learning control}

The recent success in machine learning, in particular with reinforcement learning (RL), provides another alternative to model-based optimal control. My thesis work has been strongly influenced by ideas from reinforcement learning, especially the notion of state-action space. For readers less familiar with this field, I will introduce the essential topics needed to understand and appreciate this thesis.

\subsection{Reinforcement learning is model-free optimal control}
At its core, reinforcement learning (RL) is based on Bellman updates and uses the same key insight as dynamic programming and optimal control: the \emph{principle of optimality}.
For the interested reader, I recommend comparing two formulations of value iteration: the one introduced in chapter 4.4 of the classical RL textbook by~\textcite{sutton2018book}, and the one introduced in chapter 5.3 of the classical optimal control textbook by~\textcite{bertsekas2017book}. Aside from notation and a minor detail\footnote{In~\cite{sutton2018book}, there is also a discount factor that is introduced a few pages after the initial formulation in~\cite{bertsekas2017book}.}, the equations are the same and will converge for the same reason. The critical difference is the presence (or absence) of a model, which dictates how updates can be applied.
In the model-based case, the transitions and their associated costs are known, and each iteration can make a batch update for every state. The most straightforward course is to back out updates from the goal state. In the model-free case, the cost of a specific transition can only be discovered through experience. \par
Since the cost associated with a transition cannot be known a priori by consulting a model, it is convenient to keep track of this cost by associating it to a given state-action pair, instead of just the state. Reasoning on not just the cost, but the value of a state-action pair leads to one of the breakthroughs in RL: $q$-learning~\cite[cf. chapter 6.5]{sutton2018book}. A major advantage of this approach is that the optimal value function is not specific to a given optimal policy: if the optimal policy is not unique, all optimal policies result in the same optimal value function. I will use this idea in~\cite{heim2019beyond} to quantify the robustness of a system without pre-specifying a policy.

\subsection{The challenge of robot learning}

\textcite{kober2013reinforcement} have written a comprehensive overview of RL for robotics. Although slightly dated, the primary challenges (\cite[see 8.1 in]{kober2013reinforcement}) have not changed considerably. Many core challenges stem from the difficulty of obtaining samples from real-world experiments. \par
Perhaps the most obvious difficulty is that, unlike in simulations, hardware experiments can only run at physical time, and are difficult to parallelize. Although there have been some efforts to learn control policies with a large number of robots~\cite{levine2018learning}, these tend to be very limited in scale (14 robots in~\cite{levine2018learning}), and do not provide the amount of data conventional machine learning approaches are designed for.
Hardware experiments tend to require large amounts of space and funding; unlike for simulations, the cost per experiment does not decrease dramatically with scale.
A more fundamental bottleneck is user time: a user typically needs to set up each run, reset it after failures, and maintain and repair the robots. \par
Models can help in two ways: by allowing learning in simulation, and by informing the structure of the control policy.
 \par
Models allow simulations to be used to bootstrap the learning process. Collecting data in simulation is cheap, and the policy can then be refined on the real robot. However, the transfer from simulation to the real-world is not always straightforward.
The models used in simulation are often insufficiently accurate, especially for legged locomotion~\cite{neunert2017off,tan2018sim}, and the learned policies often over-fit to these models. In other words, they find policies that may be optimal (or at least work reasonably well) but are utterly not robust. This is commonly known as the \emph{sim-to-real} problem. \par
A solution to this is dynamics randomization~\cite{peng2017sim,lowrey2018reinforcement,tan2018sim}: the parameters of the simulation are perturbed in each learning iteration, with the hope that the resulting control policy is robust to differences in the model. This approach shows promise. However, to the best of my knowledge there is no principled approach to choose which parameters to perturb, or by how much. \par
Another approach is to ensure your simulation is accurate:~\textcite{hwangbo2019learning} train a deep neural network to simulate the motor dynamics, which are typically difficult to model with first principles. A policy trained in simulation using this high-fidelity simulation, and transferred to the robot\footnote{Dynamics randomization is also used in this study, but the core novelty is the use of supervised learning to obtain an accurate simulation.}. \par
In all of these approaches, one of the most critical feedback loops is the designer, typically a graduate student.~\textcite{xie2019cassie} describe in substantial detail the iterations involved in training a policy in simulation, which they eventually deploy successfully to the Cassie biped, without any dynamics randomization. \par
Each of these approaches addresses the same fundamental issue: the need for robustness. \par
%Often, these simulation-based policies would cause critical damage to the robot and surroundings if transferred directly. 
Model-based control can also be used directly for controller design, effectively warm-starting the learning process. In some of the most straightforward approaches, tools from conventional control theory and learning control are combined but kept distinct. For example,~\textcite{kumar2018improving} design an initial stabilizing LQR-based balance and stepping controller based on a linear inverted pendulum model. A deep neural network which outputs offsets to the joint torques and target angles is overlayed on top of the model-based controller.
In another example, ~\textcite{yeganegi2019robust} start with a standard hierarchical control setup: a trajectory optimization problem is formulated using a linear inverted pendulum model to generate CoM trajectories, and a whole-body controller tracks these trajectories. However, how robustly these trajectories can be tracked depends largely on the coefficients of the cost-function used in the high-level controller. A Bayesian optimization scheme is then used to search for cost coefficients which result in trajectories that are both robust and fast. \par
Conventional control theory can also inform the design of proper policy parameterization. An appropriately chosen parameterization has a significant effect on the size of the search space as well as the resulting stability of the closed-loop system~\cite{roberts2011feedback}. Indeed, directly learning a policy which outputs motor torques is often ineffective, especially for systems that are dynamically unstable~\cite{peng2017learning}. In recent work,~\textcite{viereck2018learning} design a policy which outputs trajectories of desired states and stabilizing feedback gains. These trajectories are more robust and reliable than directly learning a state-to-torque mapping~\cite{viereck2018learning}.
In another example,~\textcite{marco2017design} incorporate the structure of a linear-quadratic regulator (LQR) into the kernel of a Gaussian process (GP). Extrapolation from previously observed samples is also more accurate. \par
One of the early promises of reinforcement learning is that very sparse rewards could be given based on task-completion. In recent years, the community has come to realize that designing an appropriately rich reward function is not a necessary evil, but rather a powerful tool. Though excessive massaging of a reward function can heavily bias the learning outcome, it also helps convergence tremendously by \emph{shaping} the reward landscape. In effect, the purpose of this proxy reward function is to allow the learning agent to sample gradient information more reliably. An effective compromise is to use this proxy only temporarily: once the learning agent has converged to a reasonable starting policy, it can continue learning without the assistance of the more informative reward function. This is called \emph{curriculum learning}~\cite{bengio2009curriculum,karpathy2012curriculum}. \par
While there is empirical evidence that this approach can be very effective, it also requires a lot of intuition and experience to design effective and general curricula. This may require a lot of insight into both the system and the task at hand, partially defeating the purpose of learning the control policy instead of designing it. There is great interest recently in automating the design of curricula, either by modeling student progress as a separate optimization problem~\cite{portelas2019teacher}, or allowing the student itself some control over the task difficulty~\cite{klink2019self}. The work of~\textcite{kumar2018improving}, mentioned above, also incorporates an adaptive curriculum by applying perturbations close to the edge of the currently known region of attraction. By collecting samples near the edge of stability, these samples are very likely to be both highly informative and remain safe.~\textcite{ivanovic2019barc} formally combine concepts from curriculum learning and backreachability to learn from a sparse reward signal effectively. In this case, the learning agent begins training near the target goal, such that even a random walk is likely to sample the reward. In subsequent iterations, the agent is initialized from a successively larger set of initial states. Instead of increasing this set arbitrarily, it is increased by computing an approximate backward reachable set over a short time horizon. In this manner, the curriculum directly takes the dynamics of the system into consideration.