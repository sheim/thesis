% !TEX root = ../root.tex

\chapter{Preliminaries} \label{chap:prelims}
This chapter introduces relevant background information to help readers from potentially different backgrounds to quickly gain enough knowledge to understand the context, relevance, and importance of the contributions of each paper. Covered are the basics of legged locomotion, reinforcement learning, and viability theory.

\section{Dynamic Legged Locomotion}
While most results in my publications are valid for dynamical systems in general, the motivation is grounded in legged locomotion, in particular running. This field offers abundant natural inspiration, including ourselves. Legged systems also encapsulate many interesting challenges in dynamics: the dynamics of legged systems are non-smooth, due to impacts at foot touchdown. They are hybrid, as the governing equations of motion switch abruptly every time a foot touches down or lifts off. They are underactuated, due to the floating base. And of course, they are typically also highly nonlinear. While passively stable legged systems exist, for most systems of interest to us, they are passively unstable and require feedback control.
On top of all this, the community of dynamic legged locomotion is both vibrant and friendly. In this section, I will give a quick introduction to the field of legged locomotion.

\subsection{Reduced Order Models of Legged Locomotion}

Reduced order models have been a cornerstone to research in legged locomotion, allowing researchers to cope with the complexity of legged locomotion dynamics. \par
In this thesis, we will focus on the spring-loaded inverted pendulum (SLIP) model for running. This model originated in the biomechanics community to describe center-of-mass movement of running in humans~\cite{blickhan1989spring}. Comprising of a point-mass to represent the body and a massless spring to represent the leg, the model is parsimonious in its parameters, making it relatively easy to fit to data.
Several studies have then fit it to various animals of different sizes and with different numbers of legs~\cite{blickhan1993similarity,daley2006running,jindrich2002dynamic}. A two-legged extension also accurately predicts ground-reaction forces of both running and walking in humans~\cite{geyer2006compliant}, convincingly showing the importance of compliance in legged locomotion.
One of the benefits of these models is that it gives a good sense of the system's \emph{natural dynamics}, or how the system naturally `wants' to move.
It is no surprise that, in parallel to the development of this model in the biomechanics community, Raibert developed his famous hopping robots using very similar models and intuition~\cite[See Figure 2.5]{raibert1986legged}.
There is a consensus that running systems should feature dynamics with compliance, and a good control scheme will \emph{exploit the natural dynamics} of the system.
This behavior is also observed in biology:~\textcite{daley2006running} used this model to explain the open-loop robustness to height perturbations observed in running birds.
Since this model is energy-conservative and neglects most degrees of freedom, it is often insufficient to describe many observations of interest. For example, extensions including actuation have been used to draw conclusions on control priorities~\cite{Birn-Jeffery3786,blum2014swing}.~\textcite{maus2015constructing} use a data-driven approach to suggest various extended state and control laws for the SLIP, based on experimental observations of humans. % Perhaps discuss this more \par

% They allow us to cope with the complexity of legged locomotion dynamics by capturing specific parts of the dynamics in simpler, low-dimensional models. They thus allow tractability and thorough analysis. \par
Reduced order models are also often used for mathematical analysis, as their low dimensionality allows for thorough investigation of the dynamics and parameters. For example,~\textcite{kuo2002energetics} analyzed `the simplest walking model', which allowed him to make statements on the energetic benefits of toe-off using simple, first-principles calculations.
In my own master thesis work~\cite{heim2016designing}, I derive the explicit equations of motion (\eom) of a running model with a tail. This is made possible by approximating the tail as a flywheel, and allows insight on scaling effects of different parameters of the tail, which could then be tested directly in hardware.
Reduced order models also allow numerical analysis by brute force, due to their low dimensionality. By making use of the \poincare section, the stability of a limit-cycle can be numerically ascertained by Floquet analysis~\cite{remy2011matlab}. Building on this approach, basins of attraction have been obtained numerically for a number of simple models~\cite{schwab2001basin,obayashi2016formation,cnops2015basin,rummel2008stable}, and in some cases also thorough bifurcations analysis~\cite{aoi2006bifurcation,merker2015stable,gan2018all}.~\textcite{byl2009metastable} used two simple models, the rimless wheel and the compass gait walker, to introduce a stochastic approach to stability, metastability, for walking systems. Metastability is a first step moving away from thinking about cyclic motion in terms of limit-cycles, by allowing the system to bounce around the limit-cycle without converging, and even diverging with probability one as time tends to infinity. Since the metastability analyses require substantial numbers of simulations, it would not be tractable on more sophisticated systems in higher dimensions. \par
% Reduced order models are also often used to explore and derive controllers. Indeed, controllers for legged robots are often split into hierarchies, with the higher levels of control reasoning on a simpler, reduced order model as compared to the lower levels of control.
Perhaps the most successful example is the linear inverted pendulum (LIP) model, commonly used for the zero-moment point algorithm~\cite{kajita2001LIP,kajita2003ZMP}. Using the same and similar models,~\textcite{koolen2012capturability} compute $n$-step capture regions: regions of foot placement for which the system can come to a complete standstill within $n$ steps. Both these approaches can be applied by making use of a hierarchical controller: these simple models are used by a high-level planner. The output of this planner is then tracked by the low-level controller using a full-order model.
Another insightful result centers around the spring-loaded inverted pendulum (SLIP) model of running.~\textcite{wu20133} show an open-loop trajectory for the swing-leg which achieves deadbeat control: any ground-height perturbation can be completely rejected in a single step. Though only seldomly applied in practice~\cite{martin2017experimental}, showing the mere existence of such an open-loop controller is impressive.
Several other studies have evaluated different controllers for simple models by brute force~\cite{piovan2013two,cnops2015basin,piovan2015reachability}.
To make use of these results in a hierarchical fashion, the low-level controller would need to track the trajectories of a SLIP model. Substantial effort has been made in this direction~\cite{hutter2010slip, wensing2013high,poulakakis2009spring, renjewski2015exciting, martin2017experimental}. However, while this approach can exploit the natural dynamics of the simple model, it may neglect or even work against the natural dynamics of the true system. My opinion is the SLIP model is an excellent model with which to gain insight of what to look for, but not a good control objective. \par

% It can be challenging to make use of these controllers in practice since they reason about the discrete dynamics defined on the \poincare section. Unlike the hierarchical controllers mentioned above, these do not output a continuous trajectory to be tracked. Instead, 


\subsection{Control for Legged Locomotion}


% Dynamics of legged locomotion include many interesting properties that make them particularly challenging to control.  \par

A recurrent theme in legged locomotion is to design controllers that can exploit the natural dynamics of the system. Indeed, despite the very rich dynamics, it is possible to achieve locomotion with very simple approaches that exploit these natural dynamics.
Perhaps the most extreme examples are those inspired by passive dynamic walkers~\cite{mcgeer1990passive}. These robots feature minimal sensing and actuation, and are typically limited to flat ground~\cite{bhounsule2012design,wisse2006design}. They make for their lack in maneuverability with extreme efficiency: the controller mostly injects small amounts of energy and then allows the natural dynamics to take their course.~\textcite{tedrake2005learning} saw in natural dynamics more than just the opportunity for efficient motion, and showed an example where the natural dynamics of a biped based on the passive dynamic walker could quickly and effectively learn and adapt an active controller. One of the key contributions of this thesis is to formalize this insight and show how the intrinsic robustness of a system's natural dynamics helps to learn control. \par
Many of the earlier successful legged robots relied on exploiting natural dynamics by using simple controllers in the form of clocks and oscillators~\cite{sprowitz2013towards,buchli2006resonance,altendorfer2001rhex,owaki2013simple}: motor positions are servoed along predetermined, periodic trajectories to the timing/phase of a clock/oscillator. All of these robots incorporate compliance in the form of mechanical springs. Once tuned to the natural dynamics, the clock and oscillator controllers generate very stable and dynamic gaits. With some feedback, the oscillators also adapt to the natural dynamics, resulting in different gaits depending on the morphology~\cite{owaki2013simple} and driving frequency~\cite{owaki2017quadruped}. % OWAKI oscillators with some feedback show adaptation to different natural dynamics
However, these approaches are often limited in versatility and typically rely on a lot of intuition and trial-and-error to design. They are also more challenging to tune for more unstable systems such as bipeds. \par
% Virtual Model Control
Optimal control offers a more explicit approach, in the form of
\begin{align*}
& \min_{u} J(x, u, t) \\
& \text{subject to } \phi(u, x, t) \leq 0,
\end{align*}
where $u$ is the control input, $x$ is the state of the system, $t$ is time, $J$ is an arbitrary, scalar cost function, and $\phi$ is a vector of constraint functions.
The structure of the dynamics can be directly incorporated into the control law in the form of constraints, such that the control law directly and explicitly considers the natural dynamics of the system.
A designer is then allowed substantial freedom to design the overall behavior by modifying the cost function.
Solving this optimal control problem is computationally expensive. It is therefore common to solve these for trajectories of a reduced order model, such as the LIP model mentioned earlier. More recently, centroidal momentum dynamics have gained popularity. It features additional dimensions which allow for more flexible trajectories~\cite{dai2014whole,koolen2016balance,ponton2016convex}, while still being dynamically consistent and computationally tractable.

Furthermore, most robots of interest are well described as rigid-body systems, and powerful and mature tools are readily available for modeling, identifying, synthesizing, and computing controllers for these systems.
% This will likely change in the future: robots with soft components~\cite{vonrohr2018gait,Buchler18ControlMusculo} or that are otherwise difficult to model accurately~\cite{surovik2018any} are starting to become more popular, and have some advantages compared to their more conventional counterparts. For the time being, 

% A designer is then allowed substantial freedom to design the overall behavior by modifying the cost function. As a result, optimal control has become one of the preeminent tools of the field, not only for control and planning~\cite{koolen2016design,ponton2016convex,winkler2018gait,mombaur_2009,deits2014footstep}, but also analysis and design~\cite{ha2018codesign,takahashi2019spring,mombaur_2009,Yesilevskiy_2018,Birn-Jeffery3786}.

\subsection{Hardware Design for Legged Locomotion}

Just as we strive for controllers that can exploit a system's natural dynamics, we will strive for hardware design that features beneficial natural dynamics.
Compliance has long been considered one of the more important features to design around.
% Even Raibert's first hopper~\cite{raibert1986legged} used a pneumatic actuator which also acted as an air-spring during stance. This approach requires a cumbersome pump.
We mentioned above several appraoches~\cite{sprowitz2013towards,buchli2006resonance,altendorfer2001rhex,owaki2013simple} which used highly-geared motors with and position control; the output of these `stiff' actuators then drove relatively soft springs, which provided the desired compliance. Among the benefits of this approach is that they allow very simple controllers to achieve remarkable stability while running on cheap hardware with low update rates (on the order of 100 Hz). The soft springs are also typically placed distally, and therefore minimize unsprung mass and protect the motors from harsh impacts. However, the complexity is traded off into the hardware design, and tuning hardware is usually much more time-consuming and expensive than tuning software. Furthermore, once the built, the parameters cannot be changed without disassembling the robot. Since different behaviors often require very different natural dynamics, this approach can severely limit a robot's versatility. \par
A different approach has been to incorporate the compliance directly in the actuator. Indeed, Raibert's first hoppers used pneumatic actuators, which act as air-springs in addition to injecting energy. They require, however, cumbersome pumps that are typically heavy and inefficient.
A popular solution are series-elastic actuators (SEA)~\cite{pratt1995series}. In this case, the output of highly-geared motors is coupled in series with a relatively stiff spring. By measuring the position of both ends of the spring, the force output at the end-effector can be trivially calculated. This allows force/torque control at the joints, and compliance of relatively arbitrary form can be generated, within the limitations of the motor constraints and kinematics. Several of the most successful robots of today are built with this type of actuation, such as ANYMAL~\cite{hutter2016anymal} and Cassie\footnote{While there are no publications on the hardware design of Cassie, this is both clear from looking at the robot and is confirmed from employees of Agility Robotics.}. The main drawbacks are added complexity for manufacturing and low-level control.
\par
A third approach eschews all mechanical springs in favor of generating compliance purely through actuation. This requires low gear-ratios in order to minimize reflected inertia and achieve transparency, while still maintaining high peak torque outputs. This results in `pancake-style' motor designs SANGBAE.
Fortunately, the widespread success of quadcopters has dramatically reduced the cost of off-the-shelf outrunner motors. Together with field-oriented control, sometimes called vector control, these motors provide a cheap solution at smaller scales. Many small and medium-sized robots capable of highly dynamic motion and direct torque-control are emerging, such as the MIT mini-cheetah~\cite{katz2019mini}, the Minitaur~\cite{kenneally2016design} and MPI's own Solo~\cite{grimminger2019open}. Since mass scales roughly cubically\footnote{Mass will scale cubically to length if we assume isometric scaling and constant, uniform density. These assumptions do not typically hold for robot designs, but do hold to some reasonable degree.}, these smaller-scale robots are mechanically sturdy~\cite{biewener2005biomechanical}, and can operate at torques that are much safer to handle. These robots are excellent experimental platforms, especially for learning control, since failures may be more common.

% Design of legged robots has come a long way in the last decade. The overall trend is to enable torque control at the joints.

% Scale: we want small robots

% Types of actuation: we want torque-control. This led to hydraulic ATLAS and HyQ, then series-elastic actuation ANYMAL, Cassie. Inspired by the haptics community, SANGBAE proprioceptive motors. Low-gear ratio allows high transparency. Commercial success of quadcopters has lowered the cost of small and mid-sized outrunner motors, allowing a new generation of small and cheap robots with torque control at the joints CITATIONS.

% This new generation of small and cheap robots the ones that most interest us. As research platforms they will allow experimentation on hardware, including model-free learning. The small size lends an inherent sturdiness to failures CITATION. The cheap production allows them to be repaired quickly and easily. 

% \subsection{Reduced-order Models of Running}

% A major challenge for these tools is still scalability. In high-dimensional systems, optimal control is often limited to finding locally optimal solutions. Alternatively, a popular approach is to split control into hierarchies. A high-level controller reasons on a reduced-order model, which becomes tractable. These solutions are then mapped to the full state-space~\cite{herzog2016momentum,dai2014whole,wensing2013high}. These reduced-order models are also particularly convenient for analysis: it is easier for us to reason about low-dimensional systems that we can easily visualize. Furthermore, they can amenable to more powerful but less tractable tools, including brute-force. \par

% It is important at this point to make a distinction between models that are useful for control and models that are useful for analysis. For control, a reduced-order model should be easy to map back into the full-order model, and provide provable guarantees. For example, the LIP assumes a constant height for walking, which does not describe natural walking well. It does however make computation tractable (convex?) and can provide guarantees, making it a good choice for control. A model for analysis should be easy to understand, and closely match the qualitative properties of interest. For example, the spring-loaded inverted pendulum (SLIP) provides very good intuitive understanding for the compliant properties of legs, both in running animals~\cite{blickhan1989spring,rummel2008stable,jindrich2002dynamic}, as well as in some robots~\cite{raibert1986legged,altendorfer2004stability}. They have also been used to conceptually explore various control concepts, such as open-loop deadbeat control~\cite{wu20133,palmer2014periodic} or reachability~\cite{piovan2015reachability}. \par
% While many efforts have been made to map these directly to the full-order model~\cite{wensing2013high,hutter2010slip,poulakakis2009spring}, typically this step requires substantial effort in terms of implementation and computation. The resulting control is typically brittle to model inaccuracies. Furthermore, forcing the high-dimensional dynamics of the actual system to behave on the low-dimensional manifold of the SLIP can potentially override the true natural dynamics. For these reasons, we recommend find the value of these models in their \emph{descriptive} role. We do not recommend their \emph{prescriptive} use to design formal controllers. \par
% Most of my work is built around the analysis of the SLIP model, since it can be reduced to a 1-dimensional state-space and 1-dimensional action-space. I would like to emphasize that we choose to use this as a \emph{descriptive} model, that is, for analysis. I do not recommend directly using it as a prescriptive model, that is, to design controllers assuming the system will behave exactly like a SLIP model. \par

\section{Learning control}

The recent success in machine learning, in particular with reinforcement learning (RL), provides another alternative to model-based optimal control. The work of my phd has been heavily influenced by ideas from reinforcement learning, especially the notion of state-action space. For readers less familiar with this field, I will introduce the essential topics needed to understand and appreciate this thesis.

% \subsection{Notes on terminology}
% Before beginning, 

\subsection{Reinforcement learning is model-free optimal control}
At its core, reinforcement learning (RL) is based on Bellman updates, and is using the same key insight as dynamic programming and optimal control: the \emph{principle of optimality}.
%In a simple discrete-time case, an update takes the form of
% \begin{equation}\label{eq:bellman}
% V(s) = r(s, a) + V(s')
% \end{equation}
% where $V$ is the value function, $r$ is the received when at state $s$ and taking action $a$, and $s'$ denotes the state at the next time-step.
% In layman terms, the principle of optimality states that, given a known optimal path from $A$ to $B$, any portion of that path is also optimal for the starting and end-points of that portion. The insight this provides is that the optimal control problem can be split into subproblems to be solved individually.
% Retruning to the Bellman updates, this allows us to consider only the immediate next step
For the interested reader, I recommend comparing two formulations of value iteration: the one introduced in chapter 4.4 of the classical RL textbook by~\textcite{sutton2018book}, and the one introduced in chapter 5.3 of the classical optimal control textbook by~\textcite{bertsekas2017book}. Aside from notation and a minor detail\footnote{In~\cite{sutton2018book}, there is also a discount factor which is introduced a few pages after the initial formulation in~\cite{bertsekas2017book}.}, the equations are the same, and will converge for the same reason. The critical difference is the presence (or absence) of a model, which dictates how updates can be applied.
In the model-based case, the transitions and their associated costs are known, and each iteration can make a batch update for every state. The most straightforward course is to back out updates from the goal state. In the model-free case, the cost of a specific transition can only be discovered through experience.
%To gather this experience, we roll out one episode, applying a policy at each step to choose the next action. Policy iteration (\cite[cf. chapter 4.3]{sutton2018book}) considers a fixed policy, and finishes with the value function of that policy. 
Since the cost associated with a transition cannot be known a priori by consulting a model, it is convenient to keep track of this cost by associating it to a given state-action pair, instead of just the state. Reasoning on not just the cost, but the value of a state-action pair leads to one of the breakthroughs in RL: $Q$-learning~\cite[cf. chapter 6.5]{sutton2018book}. A major advantage of this approach is that the value function is not specific to a given policy.. We will use this idea in~\cite{heim2019beyond} to quantify the robustness of a system without pre-specifying a policy.

\subsection{The challenge of robot learning}

\textcite{kober2013reinforcement} have written a comprehensive overview of RL for robotics. Although slightly dated, the primary challenges (\cite[see 8.1 in]{kober2013reinforcement}) have not changed considerably. Many core challenges stem from the difficulty of obtaining samples from real-world experiments. \par
Perhaps the most obvious difficulty is that, unlike in simulations, hardware experiments can only run at physical time, and are difficult to parallelize. Although there have been some efforts to learn control policies with a large number of robots~\cite{levine2018learning}, these tend to be very limited in scale (14 robots in~\cite{levine2018learning}), and do not provide the amount of data conventional machine learning approaches are designed for.
This is largely because actual hardware experiments tend to require large amounts of space and funding (the cost per experiment does not decrease with scale).
A more fundamental bottleneck is user time: a user typically needs to set up each run, reset it after failures, and maintain and repair the systems. Models can help in two ways: by allowing learning in simulation, and by informing the structure of the control policy.
 \par
Models allow simulations to be used to bootstrap the learning process. Collecting data in simulation is cheap, and the policy can then be refined on the real robot. However, the transfer from simulation to the real-world is not always straight-forward.
The models used in simulation are often insufficiently accurate, especially for legged locomotion~\cite{neunert2017off,tan2018sim}, and the learned policies often over-fit to these models. In other words, they find policies that may be optimal (or at least work reasonably well) but are utterly not robust. This is commonly known as the \emph{sim-to-real} problem. \par
A solution to this is dynamics randomization~\cite{peng2017sim,lowrey2018reinforcement,tan2018sim}: the parameters of the simulation are perturbed in each learning iteration, with the hope that the resulting control policy is robust to differences in the model. While this approach shows promise, to the best of my knowledge there is no principled approach to choose which parameters to perturb, or by how much. \par
Another approach is to ensure your simulation is accurate:~\textcite{hwangbo2019learning} use supervised learning to train a deep neural network which they use to simulate the motor dynamics, which are typically difficult to model with first principles\footnote{Dynamics randomization is also used in this study.}. This is then used to train a policy in simulation which can be transferred to the robot. \par
In all of these approaches, one of the most critical feedback loops is the designer, typically a graduate student.~\textcite{xie2019cassie} describe in substantial detail the iterations involved in training a policy in simulation, which they eventually deploy successfully to the Cassie biped, without any dynamics randomization.
Each of these approaches is addressing the same fundamental issue: the need for robustness. \par
%Often, these simulation-based policies would cause critical damage to the robot and surroundings if transferred directly. 
Model-based control can also be used directly for controller design, effectively warm-starting the learning process. In some of the most straight-forward approaches, tools from conventional control theory and learning control are combined but kept distinct. For example,~\textcite{kumar2018improving} design an initial stabilizing LQR-based balance and stepping controller based on a LIP model. This is overlayed with a deep neural network which outputs offsets to the joint torques and should target angles.
~\textcite{yeganegi2019robust} start a standard hierarchical control setup: a trajectory optimization (TO) problem is formulated using a LIP model. This generates CoM trajectories, which are then tracked with a whole-body controller. On top of this, a Bayesian optimization (BO) algorithm is used to choose weights of the TO formulation that result in trajectories that can be tracked robustly. \par
% Lin + Ponton would be another good example
% Also heijmink
Conventional control theory can also inform the design of proper policy parameterization. An appropriately chosen parameterization has significant effect on the size of the search space as well as the resulting stability of the closed-loop system~\cite{roberts2011feedback}. Indeed, directly learning a policy which outputs motor torques is often ineffective, especially for systems that are dynamically unstable~\cite{peng2017learning}. In recent work,~\textcite{viereck2018learning} design a policy which outputs a combination of desired trajectories and feedback gains to stabilize these trajectories. This allows the agent to reason at a higher level, and results in a more robust and reliable outcome than directly learning a state-to-torque mapping~\cite{viereck2018learning}.
% It is also useful to incorporate the structure of the control policy gleaned from models directly into the search space of the learning agent. This can reduce the size of the search space, and also allow more accurate extrapolation from previously gathered data. For example,
~\textcite{marco2017design} show an example incorporating structure of a linear-quadratic regulator (LQR) into the kernel of a Gaussian process (GP).
% ~\textcite{theodorou2010reinforcement} formulated the policy improvement by path integrals (PI\textsuperscript{2}), which directly leverages the structure of the optimal controller for a certain class of problems. The conditions of interest to us arre that the costs and dynamics should be quadratic and affine\footnote{Rigid-body systems are always affine with respect to torques and forces.}, respectively, with respect to the control inputs. For these systems, the Hamilton-Jacobi-Bellman equation results in a partial differential equation which can be represented as a stochastic differential equation describing the evolution of the probability distribution of 
% Read and include PI2


% Models are also useful because they can inform the structure of the policy. This can greatly reduce the search space by informing the designer of the representation the learning algorithm should operate~\cite{roberts2011feedback,peng2017learning,viereck2018learning}.

% Models are also useful because they can inform the structure of the policy. Incorporating this prior knowledge can greatly improve the learning efficacy~\cite{doerr2017model,marco2017design}. By using a representation that is compatible with a feedback controller known to have good stabilization properties also greatly increases efficacy. 


% Models are also useful, as they can inform on the structure of the policy. They tell us a lot of which space/representation the learning algorithm should operate in. This can greatly reduce the search space. \par
Finally, it has widely been recognized that reward shaping is not a necessary evil, but rather a powerful tool. Though excessive massaging of a reward function can heavily bias the learning outcome, it helps tremendously. Two main approaches to temporarily provide a richer, though potentially biased, reward are imitation learning\footnote{We do not refer to pure imitation learning, but rather mixing imitation learning with learning with experience. This is sometimes referred to as apprenticeship learning in older literature, however the field as a whole generally uses the term imitation learning in the broader sense, so we will remain consistent with this.} and curriculum learning. \par
Imitation learning is blah blah. Advantages and pitfalls. It's challenges are the assumption that the demonstrator is optimal. This is the case for model-based optimal control GPS, Viereck. This can also be used to infer safety, Glen Chou. Recent work is starting to incorporate notions of sub-optimal demonstrators, which is exciting Anca Dragan. \par
Curriculum learning is blah blah. It the main drawback is that it takes a lot of intuition to design effective and general curricula. There is interest in automating curriculum design INRIA/PETERS. There is also work leveraging backreachability for designing curricula BaRC.



\section{Viability and Backreachability}

The core of my work is based on viability theory, first pioneered by~\textcite{aubin2011viability}. The development of viability theory is largely motivated by observations of dynamical systems in nature and society, whose behavior somehow avoids chaos or pure randomness, yet never seem to settle at a resting state, an equilibrium.
Simple examples are Darwinian evolution, economics or politics of state. There is no terminal equilibrium state for these systems, or at least, none that we can identify or foresee.
The classical mathematical tools based on convergence to such an equilibrium state are therefore ill-suited to describe them, although many have found ways to adapt them to the task, for example by assuming a time-varying equilibrium state, which the system chases but never reaches. \par
Viability theory provides are more appropriate and direct description. First, sets of failure states are defined, which the system must be able to avoid. From this naturally emerges the \emph{viability kernel}, the maximal set of states from which there exist control inputs which keep the system inside the viability kernel, under the constraint of never entering the set of failure states. In simpler terms, if the system ever leaves the viability kernel, it means it can no longer return inside of it, and is doomed to eventually enter the failure set.
The viability kernel thus includes all possible regions (or "basins") of attraction, since these regions avoid the failure set by their quality of converging to a non-failing state. However, it does not require the property of convergence, and the system is free to roam the viability kernel, even stochastically, as long as it never leaves this set. \par
This provides us with a powerful way of thinking, which is particularly appropriate for considering learning systems. Indeed, although the final goal in learning control may be a control policy which generates a large region of attraction with strong convergence properties, these properties typically cannot be relied on during the learning process. \par
Viability theory therefore allows us to make very general statements of a dynamical system's behavior. However, this comes at a price: computing viable sets is typically computationally expensive, and for the types of dynamics we are most interested in it often relies on brute force. A related approach is the computation of back-reachable sets. The back-reachable set is the set of all states from which the system can reach a specified target set. This differs from convergence to equilbria, for two important distinctions. First, it does not assume convergence towards the target set, only the existence of convergence. Second, the target set does not necessarily include equilibria, and the system may not necessarily be able to remain inside the target set after reaching it. In practice, viability kernels and the back-reachable sets are largely interchangeable. For many cases of interest, they happen to coincide. Indeed, the viability kernel is in essence the back-reachable set of itself. For an introduction to methods for computing these sets, we recommend \cite{bansal2017hamilton,liniger2017real}.