% !TEX root = ../root.tex

\chapter{Preliminaries} \label{chap:prelims}
This chapter introduces relevant background information, to help readers from potentially different backgrounds to quickly gain enough knowledge to understand the context, relevance and importance of the contributions of each paper. Covered are the basics of legged locomotion, reinforcement learning and viability theory.

\section{Dynamics and Control for Legged Locomotion}
Dynamics of legged locomotion include many interesting properties that make them particularly challenging to control. They are non-smooth, due to impacts at foot touchdown. They are hybrid, as the governing equations of motion switch abruptly every time a foot touches down or lifts off. They are underactuated, due to the floating base. And of course they are typically also nonlinear. For most systems of interest to us, they are also passively unstable, and require feedback control. \par

A recurrent theme in understanding legged locomotion is to \emph{exploit the natural dynamics} of the system. Indeed, despite the very rich dynamics, it is possible to achieve locomotion with very simple approaches that exploit these natural dynamics.
The most extreme examples include passive dynamic walkers~\cite{mcgeer1990passive} and actuated robots based on them~\cite{bhounsule2012design,wisse2006design,tedrake2005learning}. \par
Making a small step in the direction of control are legged robots governed by clocks and oscillators, such as those developed by~\textcite{sprowitz2013towards,buchli2006resonance,altendorfer2001rhex,owaki2013simple}. However, these approaches are often limited in performance and versatility, and typically rely on a lot of intuition and trial-and-error. \par
Optimal control offers a more explicit approach, in the form of 
\begin{align}
& \min_{u} J(x, u, t) \\
& \text{subject to } \phi(u, x, t) \leq 0,
\end{align}
where $u$ is the control input, $x$ is the state of the system, $t$ is time, $J$ is an arbitrary, scalar cost function and $\phi$ is a vector of constraint functions. The structure of the dynamics can be directly incorporated into the control law in the form of constraints, such that the control law directly and explicitly considers the natural dynamics of the system. Furthermore, most robots of interest are well described as rigid-body systems, and powerful and mature tools are readily available for modeling, identifying, synthesizing and computing controllers for these system. A designer is then allowed substantial freedom to design the overall behavior by modifying the cost function. As a result, optimal control has become one of the preeminent tools of the field, not only for control and planning~\cite{koolen2016design,ponton2016convex,winkler2018gait,mombaur_2009,deits2014footstep}, but also analysis and design~\cite{ha2018codesign,takahashi2019spring,mombaur_2009,Yesilevskiy_2018,Birn-Jeffery3786}.

\subsection{Reduced-order Models of Running}

A major challenge for these tools is still scalability. In high-dimensional systems, optimal control is often limited to finding locally optimal solutions. Alternatively, a popular approach is to split control into hierarchies. A high-level controller reasons on a reduced-order model, which becomes tractable. These solutions are then mapped to the full state-space~\cite{herzog2016momentum,dai2014whole,wensing2013high}. These reduced-order models are also particularly convenient for analysis: it is easier for us to reason about low-dimensional systems that we can easily visualize. Furthermore, they can amenable to more powerful but less tractable tools, including brute-force. \par

It is important at this point to make a distinction between models that are useful for control and models that are useful for analysis. For control, a reduced-order model should be easy to map back into the full-order model, and provide provable guarantees. For example, the LIP assumes a constant height for walking, which does not describe natural walking well. It does however make computation tractable (convex?) and can provide guarantees, making it a good choice for control. A model for analysis should be easy to understand, and closely match the qualitative properties of interest. For example, the spring-loaded inverted pendulum (SLIP) provides very good intuitive understanding for the compliant properties of legs, both in running animals~\cite{blickhan1989spring,rummel2008stable,jindrich2002dynamic}, as well as in some robots~\cite{raibert1986legged,altendorfer2004stability}. They have also been used to conceptually explore various control concepts, such as open-loop deadbeat control~\cite{wu20133,palmer2014periodic} or reachability~\cite{piovan2015reachability}. \par
While many efforts have been made to map these directly to the full-order model~\cite{wensing2013high,hutter2010slip,poulakakis2009spring}, typically this step requires substantial effort in terms of implementation and computation. The resulting control is typically brittle to model inaccuracies. Furthermore, forcing the high-dimensional dynamics of the actual system to behave on the low-dimensional manifold of the SLIP can potentially override the true natural dynamics. For these reasons, we recommend find the value of these models in their \emph{descriptive} role. We do not recommend their \emph{prescriptive} use to design formal controllers. \par
Most of my work is built around the analysis of the SLIP model, since it can be reduced to a 1-dimensional state-space and 1-dimensional action-space. I would like to emphasize that we choose to use this as a \emph{descriptive} model, that is, for analysis. I do not recommend directly using it as a prescriptive model, that is, to design controllers assuming the system will behave exactly like a SLIP model. \par

Also bring in some biology uses, such as DALEY/HUBICKI/HAEUFLE.

\section{Learning Control}

The recent success in machine learning, in particular with reinforcement learning (RL), is pushing another alternative to conventional model-based optimal control. The work of my phd has been heavily influenced by ideas from reinforcement learning, especially the concept of state-action space and model-free control. For readers less familiar with this field, I will give a brief introduction of the concepts that are useful for understanding my work.

\subsection{Model-free and model-based}
At its core, reinforcement learning is simply model-free optimal control. Just as dynamic programming and value iteration, the standard RL algorithm is based on Bellman updates. 

Let us start with standard reinforcement learning. At its core, it is simply a model-free version of optimal control. EXAMPLE WITH VALUE ITERATION AND Q-LEARNING.
At the end of the day, all just Bellman updates. \par

While early efforts in reinforcement learning emphasized the strength of being purely model-free, it has since become clear that models are indeed useful. Although model-predictions are never perfectly accurate, they are usually still quite good, especially for the systems we are interested in. A lot of more recent effort in learning control mixes model-free and model-based tools. \par

Learn a model, then use it (sys ID).

Start with model-based tools, then learn an additional additive controller to account for un-modeled dynamics.

Use a model to generate data (simulation).

\subsection{Shaping}

In RL, shaping refers to shaping the reward-landscape, in order to make it easier to for a learning agent to learn on.
Most of the effort in this field is called curriculum design: the agent learns on easier tasks which inform it about the original policy.
There can also be shaping by modifying the reward function. This can be a temporary, handcrafted change, or it can be IRL.
It is akin to solving a convex approximation of the cost-function in model-based optimization. As RL often takes a model-free approach, usually there is no emphasis on lower and upper bounds (as there is in model-based optimization).
Mostly focused on changing $R$ in the MDP, in some cases a combination of $R$ and $P$.

% \section{Template Models}
% To explore 

\section{Viability and Backreachability}

The core of my work is based on viability theory, first pioneered by~\textcite{aubin2011viability}. The development of viability theory is largely motivated by observations of dynamical systems in nature and society, whose behavior somehow avoids chaos or pure randomness, yet never seem to settle at a resting state, an equilibrium.
Simple examples are Darwinian evolution, economics or politics of state. There is no terminal equilibrium state for these systems, or at least, none that we can identify or foresee.
The classical mathematical tools based on convergence to such an equilibrium state are therefore ill-suited to describe them, although many have found ways to adapt them to the task, for example by assuming a time-varying equilibrium state, which the system chases but never reaches. \par
Viability theory provides are more appropriate and direct description. First, sets of failure states are defined, which the system must be able to avoid. From this naturally emerges the \emph{viability kernel}, the maximal set of states from which there exist control inputs which keep the system inside the viability kernel, under the constraint of never entering the set of failure states. In simpler terms, if the system ever leaves the viability kernel, it means it can no longer return inside of it, and is doomed to eventually enter the failure set.
The viability kernel thus includes all possible regions (or "basins") of attraction, since these regions avoid the failure set by their quality of converging to a non-failing state. However, it does not require the property of convergence convergence, and the system is free to roam the viability kernel, even stochastically, as long as it never leaves this set. \par
This provides us with a powerful way of thinking, which is particularly appropriate for considering learning systems. Indeed, although the final goal in learning control may be a control policy which generates a large region of attraction with strong convergence properties, these properties typically cannot be relied on during the learning process. \par
Viability theory therefore allows us to make very general statements of a dynamical system's behavior. However, this comes at a price: computing viable sets is typically computationally expensive, and for the types of dynamics we are most interested in it often relies on brute force. A related approach is the computation of back-reachable sets. The back-reachable set is the set of all states from which the system can reach a specified target set. This differs from convergence to equilbria, for two important distinctions. First, it does not assume convergence towards the target set, only the existence of convergence. Second, the target set does not necessarily include equilibria, and the system may not necessarily be able to remain inside the target set after reaching it. In practice, viability kernels and the back-reachable sets are largely interchangeable. For many cases of interest, they happen to coincide. Indeed, the viability kernel is in essence the back-reachable set of itself. For an introduction to methods for computing these sets, we recommend \cite{bansal2017hamilton,liniger2017real}.