% !TEX root = ../root.tex

\chapter{Preliminaries} \label{chap:prelims}
This chapter introduces relevant background information to help readers from potentially different backgrounds to quickly gain enough knowledge to understand the context, relevance, and importance of the contributions of each paper, in particular as related to dynamic legged locomotion. Covered are the basics viability theory, legged locomotion, and reinforcement learning.

\section{Viability and Backreachability}

The core of my work is based on viability theory, first pioneered by Jean-Pierre Aubin~\cite{aubin2011viability}. The development of viability theory is largely motivated by observations of dynamical systems in nature and society, whose behavior somehow avoids chaos or pure randomness, yet never seem to settle at a resting state, an equilibrium.
Some examples are Darwinian evolution, economics or politics of state. There is no terminal equilibrium state for these systems, or at least, none that we can identify or foresee.
The classical mathematical tools based on convergence to such an equilibrium state are therefore ill-suited to describe them, although they can be manipulated for the purpose: for example, by by assuming a quickly time-varying equilibrium state, which the system chases but is never able to reach. \par
Viability theory provides a more appropriate and direct description. First, sets of failure states are defined, that the system must be able to avoid. We will think of this failure set as a set of absorbing states. From this naturally emerges the \emph{viability kernel}, the maximal set of states from which there exist control inputs that keep the system inside the viability kernel for all time. In simpler terms, if the system ever leaves the viability kernel, it can no longer return inside of it, and is doomed to enter the failure set within finite time. \par
Compared to mathematical objects based on the notion of convergence, such as regions of attraction~\cite[(section 6.4)]{strogatz2018nonlinear} or contractions~\cite{bazzi2018stability}, conditions for viability are weaker (they provide no convergence properties) but more general. \par
Two other related sets which depart from the notion of convergence are \emph{backreachable sets}~\cite{bansal2017hamilton} and \emph{controllable sets}~\cite{zaytsev2018boundaries}. For backreachable sets, we start by defining a target set.
The backreachable set is the set of states from which there exist control inputs which guide the system into the target set within finite time.
Since being able to reach this target set directly implies being able to avoid the failure set, the back-reachable set is always a subset of the viability kernel.
Controllable sets require that every point in the set is reachable from anywhere in the set. In other words, the controllable set must be a backreachable set for all subsets of the controllable set itself. Controllability, sometimes referred to as ergodicity in the context of Markov decision processes~\cite{moldovan2012safemdp}, is in a way a more general statement than backreachability (it does not require the definition of a target state), but also a stronger statement: not all backreachable sets are controllable sets. By the same quality of backreachable sets, controllable sets are also subsets of the viability kernel. \par
Viability theory therefore allows us to make very general statements of a dynamical system's behavior, without making many assumptions or conditions. This generality comes at a price: computing viable sets is typically computationally expensive. For the types of dynamics we are interested in, we will typically resort to gridding and brute force. In this thesis, we will use the concepts of viability theory, but will not concern ourselves with algorithms that scale to higher dimensions. For the interested reader, I recommend starting with~\cite{liniger2017real} for a clear and practical application,~\cite{deffuant2007approximating} for an example of sample-driven approximations, and~\cite{bansal2017hamilton} for an overview of tools for the computation of the related backreachable sets.

\subsection{Why use viability for legged locomotion?}

The study of legged locomotion has come a long way by thinking of gaits as stable limit-cycles: periodic orbits through state space towards which nearby orbits converge. This view allows us to explain various observations with a well developed mathematical language: nonlinear dynamics and bifurcations~\cite{strogatz2018nonlinear}. \par
For example, the remarkable stability of running dynamics~\cite{daley2006running} can be understood by studying basins of attraction of a specific gait~\cite{merker2015stable,cnops2015basin}. The choice and transition between different gaits depending on locomotion speed or morphology can be understood via bifurcation studies~\cite{owaki2013simple,aoi2013stability,gan2018all}. \par

However, these tools hinge on the existence of a limit-cycle, and therefore are ill-suited for understanding unsteady or erratic motion, which is also commonly observed~\cite{wheatley2015escape,moore2017unpredictability}.
Even when it is reasonable to assume the existence of a limit-cycle, for example when walking or running on a treadmill at steady speed, it is often difficult to fit recorded time-series data to these assumptions~\cite{bruijn2013assessing, maus2015constructing}. To gain further insight on locomotion, we need tools which can reason about stability in the sense of avoiding falls instead of in the sense of convergence~\cite{Birn-Jeffery3786}. \par

Viability theory provides a natural description for this type of stability. Furthermore, viability makes no assumptions on the control policy or the task at hand. This makes it appropriate for analyzing morphology separately from the control policy, which we will do in~\cite{heim2019beyond}. \par

In the field of legged robotics and control, there have also been developments which move away from the perspective of strict convergence.~\textcite{byl2009metastable} relax the requirement of convergence with the probabilistic concept of \emph{metastability}, and compute the mean first passage of time for several simple models of legged locomotion. This metric takes into account the probability of failure from any sources of uncertainty, and can effectively serve as a metric of task-level robustness.
However, the structure of how dynamics relate to failures is not exposed. Furthermore, changing any source of uncertainty, such as distribution of disturbances, requires the mean first-passage of time to be recomputed from scratch. \par

Capture points and capture regions~\cite{koolen2012capturability} build on the ideas of backreachability to find effective yet simple foot placements that can come to a standstill. In this context, a \emph{capture point} represents a foot placement which takes the system to a \emph{captured state}: standing, static equilibrium. An \emph{$n$-step capture region} is the set of foot placements which take the system to a state, from which the captured state can be reached within $n-1$ steps.
The core idea is that it isn't necessary for a controller to always seek convergence, as long as the system stays within a region from which it can converge within a finite number of steps.
Capture regions are particularly useful in legged locomotion, since they are typically easier to compute than viability kernels, and usually as large or nearly so~\cite{koolen2012capturability,zaytsev2018boundaries}. The core results of this thesis, namely~\cite{heim2019beyond,heim2019learnable}, can be reformulated to use capture regions instead of viable sets, without much loss of precision. We find, nonetheless, the more exact language of viability theory to be useful in formalizing the mathematical objects developed in this thesis. \par



% The viability kernel thus includes all possible regions (or "basins") of attraction, since these regions avoid the failure set by their quality of converging to a non-failing state. However, it does not require the property of convergence, and the system is free to roam the viability kernel, even stochastically, as long as it never leaves this set. \par
% This provides us with a powerful way of thinking, which is particularly appropriate for considering learning systems. Indeed, although the final goal in learning control may be a control policy which generates a large region of attraction with strong convergence properties, these properties typically cannot be relied on during the learning process. \par
% Viability theory therefore allows us to make very general statements of a dynamical system's behavior. However, this comes at a price: computing viable sets is typically computationally expensive, and for the types of dynamics we are most interested in it often relies on brute force. A related approach is the computation of back-reachable sets. The back-reachable set is the set of all states from which the system can reach a specified target set. This differs from convergence to equilbria, for two important distinctions. First, it does not assume convergence towards the target set, only the existence of convergence. Second, the target set does not necessarily include equilibria, and the system may not necessarily be able to remain inside the target set after reaching it. In practice, viability kernels and the back-reachable sets are largely interchangeable. For many cases of interest, they happen to coincide. Indeed, the viability kernel is in essence the back-reachable set of itself. For an introduction to methods for computing these sets, we recommend \cite{bansal2017hamilton,liniger2017real}.

\section{Dynamic legged locomotion}
While most results in my publications are valid for arbitrary dynamical systems, the motivation is grounded in legged locomotion, in particular running. This field offers abundant natural inspiration, including ourselves. Legged systems also encapsulate many interesting challenges in dynamics: the dynamics of legged systems are non-smooth, due to impacts at foot touchdown. They are hybrid, as the governing equations of motion switch abruptly every time a foot touches down or lifts off. They are underactuated, due to the floating base. And of course, they are typically highly nonlinear. While passively stable legged systems exist, for most systems of interest to us, they are passively unstable and require feedback control.
On top of all this, the community of dynamic legged locomotion is both vibrant and friendly. In this section, I will give a succinct introduction to the field.

\subsection{Reduced order models of legged locomotion}

Reduced order models allow researchers to cope with the complexity of legged locomotion dynamics. \par
In this thesis, we will focus on the spring-loaded inverted pendulum (SLIP) model for running. This model originated in the biomechanics community to describe center-of-mass movement of running in humans~\cite{blickhan1989spring}. Comprising of a point-mass to represent the body and a massless spring to represent the leg, the model is parsimonious in its parameters, making it relatively easy to fit to data.
Several studies have then fit it to various animals of different sizes and with different numbers of legs~\cite{blickhan1993similarity,daley2006running,jindrich2002dynamic}. A two-legged extension also accurately predicts ground-reaction forces of both running and walking in humans~\cite{geyer2006compliant}. These studies of spring-mass models have convincingly showed the importance of compliance in legged locomotion. \par
One of the benefits of these models is that it gives a good sense of the system's \emph{natural dynamics}, or how the system naturally `wants' to move.
It is no surprise that, in parallel to the development of this model in the biomechanics community, Raibert developed his famous hopping robots using very similar models and intuition~\cite[see Figure 2.5]{raibert1986legged}.
There is a consensus that running systems should feature dynamics with compliance, and a good control scheme will \emph{exploit the natural dynamics} of the system.
This behavior is also observed in biology:~\textcite{daley2006running} used this model to explain the open-loop robustness to height perturbations observed in running birds. \par
However, since this model is energy-conservative and neglects most degrees of freedom, it is often insufficient to describe many observations of interest. For example, extensions including actuation have been used to draw conclusions on control priorities~\cite{Birn-Jeffery3786,blum2014swing}. In another example,~\textcite{maus2015constructing} use a data-driven approach to suggest various extended state and control laws for the SLIP, based on experimental observations of humans. % Perhaps discuss this more \par

% They allow us to cope with the complexity of legged locomotion dynamics by capturing specific parts of the dynamics in simpler, low-dimensional models. They thus allow tractability and thorough analysis. \par
Reduced order models are also often used for mathematical analysis, as their low dimensionality allows for thorough investigation of the dynamics and parameters.
% For example,~\textcite{kuo2002energetics} analyzed `the simplest walking model', which allowed him to make statements on the energetic benefits of toe-off using simple, first-principles calculations.
In my own master thesis work~\cite{heim2016designing}, I derive the explicit equations of motion (\eom) of a running model with a tail. This is made possible by approximating the tail as a flywheel, and allows insight on scaling effects of different parameters of the tail, which could then be tested directly in hardware. \par
Reduced order models also allow numerical analysis by brute force, due to their low dimensionality. The basins of attraction for many such models have been studied numerically~\cite{schwab2001basin,obayashi2016formation,cnops2015basin,rummel2008stable}, and extended in some cases to bifurcation analysis~\cite{aoi2006bifurcation,merker2015stable,gan2018all}. By making use of the \poincare section, the stability of a limit-cycle can be numerically ascertained by Floquet analysis~\cite{remy2011matlab}.
Reduced order models are also used by~\textcite{byl2009metastable}, since computing the mean first-passage of time requires a very large number of simulations and would be prohibitive for more sophisticated models. \par
These simple models have also been used to study various control concepts~\cite{piovan2013two,cnops2015basin,piovan2015reachability}. In one of the most insightful examples, ~\textcite{wu20133} show an open-loop trajectory for the swing-leg which achieves deadbeat control: any ground-height perturbation can be completely rejected in a single step.
% The work on metastability by~\textcite{byl2009metastable} relies on the mean first-passage of time, which is very computationally heavy. 
% The work on metastability by~\textcite{byl2009metastable}, mentioned in the previous Section, is . Metastability is a first step moving away from thinking about cyclic motion in terms of limit-cycles, by allowing the system to bounce around the limit-cycle without converging, and even diverging with probability one as time tends to infinity. Since the metastability analyses require substantial numbers of simulations, it would not be tractable on more sophisticated systems in higher dimensions. \par
% Reduced order models are also often used to explore and derive controllers. Indeed, controllers for legged robots are often split into hierarchies, with the higher levels of control reasoning on a simpler, reduced order model as compared to the lower levels of control.
Leaving the world of spring-mass models of running, one of the most successful reduced order models is the linear inverted pendulum  model, commonly used for the zero-moment point algorithm~\cite{kajita2001LIP,kajita2003ZMP}. Using the same and similar models,~\textcite{koolen2012capturability} compute $n$-step capture regions: regions of foot placement for which the system can come to a complete standstill within $n$ steps. Both these approaches can be applied by making use of a hierarchical controller: these simple models are used by a high-level planner. The output of this planner is then tracked by the low-level controller using a full-order model.
% For running locomotion, reduced order models often center around the spring-loaded inverted pendulum (SLIP) model of running.~\textcite{wu20133} show an open-loop trajectory for the swing-leg which achieves deadbeat control: any ground-height perturbation can be completely rejected in a single step.
%Though only seldomly applied in practice~\cite{martin2017experimental}, showing the mere existence of such an open-loop controller is impressive.
% Several other studies have evaluated different controllers for simple models by brute force~\cite{piovan2013two,cnops2015basin,piovan2015reachability}.
%To make use of these results in a hierarchical fashion, the low-level controller would need to track the trajectories of a SLIP model. Substantial effort has been made in this direction~\cite{hutter2010slip, wensing2013high,poulakakis2009spring, renjewski2015exciting, martin2017experimental}. However, while this approach can exploit the natural dynamics of the simple model, it may neglect or even work against the natural dynamics of the true system. My opinion is the SLIP model is an excellent model with which to gain insight of what to look for, but not a good control objective. \par

% It can be challenging to make use of these controllers in practice since they reason about the discrete dynamics defined on the \poincare section. Unlike the hierarchical controllers mentioned above, these do not output a continuous trajectory to be tracked. Instead, 


\subsection{Heuristic Control for legged locomotion}


% Dynamics of legged locomotion include many interesting properties that make them particularly challenging to control.  \par

A recurrent theme in legged locomotion is to design controllers that can exploit the natural dynamics of the system. Indeed, despite complicated dynamics, it is possible to achieve locomotion with very simple approaches that exploit these natural dynamics.
Perhaps the most extreme examples are those inspired by passive dynamic walkers~\cite{mcgeer1990passive}. These robots feature minimal sensing and actuation, and are typically limited to flat ground~\cite{bhounsule2012design,wisse2006design}. They make up for their lack in maneuverability with extreme efficiency: the controller mostly injects small amounts of energy and then allows the natural dynamics to take their course.~\textcite{tedrake2005learning} saw in natural dynamics more than just the opportunity for efficient motion: they use a biped robot based on the passive dynamic walkers, and show it can efficiently and reliably learn and adapt an active controller. One of the key contributions of this thesis is to formalize this insight and show how the intrinsic robustness of a system's natural dynamics helps to learn control. \par
Many of the earlier successful legged robots relied on exploiting natural dynamics by using simple controllers in the form of clocks and oscillators~\cite{sprowitz2013towards,buchli2006resonance,altendorfer2001rhex,owaki2013simple}: motor positions are servoed along predetermined, periodic trajectories to the timing/phase of a clock/oscillator. All of these robots incorporate compliance in the form of mechanical springs. Once tuned to the natural dynamics, the clock and oscillator controllers generate very stable and dynamic gaits. With some feedback, the oscillators also adapt to the natural dynamics, resulting in different gaits depending on the morphology~\cite{owaki2013simple} or driving frequency~\cite{owaki2013simple,owaki2017quadruped}. % OWAKI oscillators with some feedback show adaptation to different natural dynamics
However, these approaches are often limited in versatility and typically rely on a lot of intuition and trial-and-error to design. They are also more challenging to tune for inherently unstable systems such as bipeds. \par
Another approach is to use a hierarchical control scheme, in which the low-level controller causes the system to behave like a reduced order model, such as the SLIP model discussed above.
If this low-level controller is accurate enough, the plethora of control laws studied for the SLIP model can be directly leveraged. To this end, there have been many efforts to design such a low-level controller~\cite{hutter2010slip,poulakakis2009spring,wensing2013high}. Unlike for the linear inverted pendulum model, however, it is not trivial to map between the high-level representation of a SLIP model and low-level representation of an actual robot, and this approach has only seldomly been applied in practice~\cite{martin2017experimental}.
Furthermore, these high-level controllers \emph{exploit the natural dynamics of the reduced order model}, but not necessarily those of the system. It is entirely possible for the actual natural dynamics of the system to be negated by the low-level controllers.
For these reasons, I believe reduced order models such as the SLIP are excellent \emph{descriptive} models but poor \emph{prescriptive} models: they allow a deep understanding, but make for poor control targets. \par
Virtual model control~\cite{pratt2001virtual,renjewski2015exciting} provides a slightly more relaxed approach: the low-level controller simply commands torques to mimic a spring-damper between two arbitrary points, usually between the hip and the foot. This approach doesn't force the system's dynamics onto the submanifold of the SLIP model, but retains the complaint behavior. It is also easy to implement, requiring only knowledge of the kinematics. However, this approach does not capture the impedance of the system, and it can be difficult to stabilize the robot trunk in more aggressive motion.

\subsection{Optimal Control for legged locomotion}
Optimal control offers a more explicit approach to consider the natural dynamics in the controller.
The setting is to find a policy $u_k = \pi(x_k)$  which minimizes the total cost-to-go function $J$ given a dynamical system $x_{k+1} = f(x_k, u_k)$:
\begin{align*}
\text{find } & u_k = \pi(x_k) \\
\text{such that } & J^*(x_0) = \min_u \left[g_N(x_N) + \sum_{k=0}^{N-1}g(x_k, u_k)\right] \\
\text{subject to } & x_{k+1} = f(x_k, u_k)
\end{align*}
where $J*$ is the optimal cost accumulated between any state $x_0$ and a terminal state $x_N$, $g_N$ is the final cost of being in $x_N$, and $g(x_k, u_k)$ is the cost incurred for applying the control input $u_k$ from state $x_k$.
This formulation explicitly takes the dynamics of the system into consideration, since the solution must be consistent with the dynamics $f(x_k, u_k)$.
One of the core insights in optimal control is the \emph{principle of optimality}: the fact that any subtrajectory of an optimal trajectory is itself optimal for its own starting and ending states.
The optimal control problem can therefore be to be split into several small steps, and each of these can be solved individually.
This principle is leveraged in the Bellman equation (or its continuous-time equivalent, the Hamilton-Jacobi-Bellman equation):
\begin{equation}\label{eq:bellman}
J^*(x_k) = \min_u\left[g(x_k,u_k) + J^*(f(x_k, u_k)) \right].
\end{equation}
If this function is known, finding an optimal controller is reduced to a one-step lookahead optimization.
To find $J^*$, we can simply use the Bellman equation as an update rule, which leads to the \emph{dynamic programming algorithm}. A typical update will start at the terminal state $x_N$ (for which the cost-to-go is trivial to compute), and then apply these updates by backing out from the terminal state. This process is repeated until $J^*$ has converged. Once $J^*$ has converged, a control policy which greedily follows eq. \ref{eq:bellman} will be optimal from any state.
The main caveat with this approach is that it does not scale well with dimensionality. \par
Perhaps the most popular optimal control approach used in legged locomotion today is trajectory optimization: in this setting, we give up on global optimality and instead seek trajectories which are only locally optimal.
For the interested reader, I recommend~\cite{kelly2017introduction}. Without going into details, there are two main families of trajectory optimization: shooting methods and transcription methods.
\par
Shooting methods iterate making a forward pass, using an initial guess to simulate a trajectory, and then a backward pass to compute updates to the control inputs used.
A popular and fast shooting method is differential dynamic programming~\cite{tassa2011theory}, in which each backward pass updates the control trajectory with a Bellman update using a local, quadratic approximation of the cost-to-go function. This allows the updates to be backed out of the final state, in a conceptually similar way as the dynamic programming algorithm.
Other shooting methods will typically pass the entire trajectory to a general nonlinear programming (NLP) solver, which optimizes over the entire trajectory at once~\cite{kelly2017introduction}. This approach tends to struggle with problems that require complex controls\footnote{I will not attempt a rigorous definition of what more or less 'complex' controls are. Suffice it to say that legged locomotion typically requires controls which are more complex than orbital dynamics of satellites.}, such as legged locomotion. Nonetheless, it has been used successfully for offline design and analysis~\cite{mombaur_2009,remy2011matlab}. \par
% One example is \emph{differential dynamic programming}~\cite{tassa2011theory}; during the backwards pass we compute a local, second order approximation of the cost-to-go function along the simulated trajectory. Then, a one-step Bellman update can be applied to correct the trajectory of control inputs, backing out of the final state in a similar fashion to dynamic programming. The updated trajectory of controls can then be used to make another forward pass, and so on until convergence. An important advantage compared to other methods is that the update of the cost-to-go function includes a feedback term to stabilize the trajectory. Other shooting methods will typically pass the entire trajectory to a general nonlinear programming (NLP) solver, which optimizes over the entire trajectory at once~\cite{kelly2017introduction}. This approach tends to struggle with problems that require complex controls\footnote{I will not attempt a rigorous definition of what more or less 'complex' controls are. Suffice it to say that legged locomotion typically requires controls which are more complex than orbital dynamics of satellites.}, such as legged locomotion. \par
Transcription methods break up the problem completely, and allow an NLP solver to optimize not only over the control inputs at each time step, but also the states. Dynamic consistency is then enforced by transcribing the dynamics into constraints the solver must satisfy~\cite{kelly2017introduction}. \par
An important challenge for all these methods is the presence of contacts in legged locomotion. Making or breaking a contact induces a non-smooth jump in the cost, which solvers struggle with.
While there are some formulations that can directly reason about contacts~\cite{mordatch2012discovery,dai2014whole,winkler2018gait}, these tends to be slow. A common approach is to split the problem into parts which can be solved separately in a hierarchical control scheme. Typically, these will include generating a center of mass (and sometimes momentum) trajectory~\cite{dai2014whole,buchli2009compliant}, a sequence of footstep positions and timings~\cite{buchli2009resonance,deits2014footstep}, and a whole-body controller to execute the first two~\cite{sentis2007synthesis,buchli2009compliant}. Much of the current research aims at improving the robustness~\cite{,manchester2019robust,yeganegi2019robust}, speed~\cite{ponton2018ontime,lin2019efficient}, or flexibility~\cite{boussema2019impulse} of these individual problems.

\subsection{Hardware Design for legged locomotion}

Just as we strive for controllers that can exploit a system's natural dynamics, we strive for hardware design that features beneficial natural dynamics.
It has long been considered important to design legged systems around the concept of compliance.
We mentioned above several designs~\cite{sprowitz2013towards,buchli2006resonance,altendorfer2001rhex,owaki2013simple} which use highly-geared motors and position control; the output of these `stiff' actuators then drive relatively soft springs, which provide the desired compliance. The soft springs are typically placed distally, and therefore minimize unsprung mass and protect the motors from harsh impacts. Additionally, simple controllers can achieve remarkable stability while running on cheap hardware with low update rates (on the order of 100 Hz). However, the complexity is traded off into the hardware design, and tuning hardware is usually much more time-consuming and expensive than tuning software. Furthermore, once built, the parameters usually cannot be changed without disassembling the robot. Since different behaviors often require very different natural dynamics, this approach can severely limit a robot's versatility. \par
A second approach is to incorporate the compliance directly in the actuators. Indeed, Raibert's first hoppers used pneumatic actuators, which act as air-springs in addition to injecting energy~\cite{raibert1986legged}. They require, however, cumbersome pumps that are typically heavy and inefficient.
A popular solution are series-elastic actuators (SEA)~\cite{pratt1995series}. In this case, the output of highly-geared motors is coupled in series with a relatively stiff spring. By measuring the position of both ends of the spring, the force output at the end-effector can be directly calculated. This allows force/torque control at the joints, and compliance of relatively arbitrary form can be generated, within the limitations of the motor constraints and kinematics. Several of the most successful robots of today are built with this type of actuation, such as ANYMAL~\cite{hutter2016anymal} and Cassie\footnote{While there are no publications on the hardware design of Cassie, this is both clear from looking at the robot and is confirmed from employees of Agility Robotics.}. ATRIAS, the predecessor to Cassie, was even designed deliberately to be amenable to control schemes designed around the SLIP model~\cite{hubicki2016atrias}. The main drawbacks are added complexity for manufacturing and low-level control.
\par
A third approach eschews any mechanical springs in favor of generating compliance purely through actuation and control. This requires low gear-ratios in order to minimize reflected inertia and achieve transparency, while still maintaining high peak torque outputs. This results in `pancake-style' motor designs~\cite{Seok2012proprioceptive}.
Fortunately, the widespread success of quadcopters has dramatically reduced the cost of off-the-shelf outrunner motors which fit these requirements. Together with field-oriented control, sometimes called vector control, these motors provide a cheap solution at smaller scales. Many small and medium-sized robots capable of highly dynamic motion and direct torque-control are emerging, such as the MIT mini-cheetah~\cite{katz2019mini}, the Minitaur~\cite{kenneally2016design} and MPI-IS's own Solo~\cite{grimminger2019open}. Since mass scales roughly cubically\footnote{Mass will scale cubically to length if we assume isometric scaling and constant, uniform density. While robot designs clearly break these assumptions, this serves as a good rule of thumb.}, these smaller-scale robots are mechanically sturdy~\cite{biewener2005biomechanical}, and can operate at torques that are much safer to handle. These robots are excellent experimental platforms, especially for learning control, since failures may be more common. \par

% Regardless of the approach taken, hardware design has remained more an art than a science, with experience and skill of the designer playing a crucial role. An attractive alternative is \emph{co-design}, or allowing an algorithm to simultaneously design both the morphology and controller of the robot. This idea was first pioneered by~\textcite{sims1994evolving}, who used evolutionary algorithms to search over the parameter space of both morphology and controllers simultaneously. More recent work typically makes use of trajectory optimization

% Design of legged robots has come a long way in the last decade. The overall trend is to enable torque control at the joints.

% Scale: we want small robots

% Types of actuation: we want torque-control. This led to hydraulic ATLAS and HyQ, then series-elastic actuation ANYMAL, Cassie. Inspired by the haptics community, SANGBAE proprioceptive motors. Low-gear ratio allows high transparency. Commercial success of quadcopters has lowered the cost of small and mid-sized outrunner motors, allowing a new generation of small and cheap robots with torque control at the joints CITATIONS.

% This new generation of small and cheap robots the ones that most interest us. As research platforms they will allow experimentation on hardware, including model-free learning. The small size lends an inherent sturdiness to failures CITATION. The cheap production allows them to be repaired quickly and easily. 

% \subsection{Reduced-order Models of Running}

% A major challenge for these tools is still scalability. In high-dimensional systems, optimal control is often limited to finding locally optimal solutions. Alternatively, a popular approach is to split control into hierarchies. A high-level controller reasons on a reduced-order model, which becomes tractable. These solutions are then mapped to the full state-space~\cite{herzog2016momentum,dai2014whole,wensing2013high}. These reduced-order models are also particularly convenient for analysis: it is easier for us to reason about low-dimensional systems that we can easily visualize. Furthermore, they can amenable to more powerful but less tractable tools, including brute-force. \par

% It is important at this point to make a distinction between models that are useful for control and models that are useful for analysis. For control, a reduced-order model should be easy to map back into the full-order model, and provide provable guarantees. For example, the LIP assumes a constant height for walking, which does not describe natural walking well. It does however make computation tractable (convex?) and can provide guarantees, making it a good choice for control. A model for analysis should be easy to understand, and closely match the qualitative properties of interest. For example, the spring-loaded inverted pendulum (SLIP) provides very good intuitive understanding for the compliant properties of legs, both in running animals~\cite{blickhan1989spring,rummel2008stable,jindrich2002dynamic}, as well as in some robots~\cite{raibert1986legged,altendorfer2004stability}. They have also been used to conceptually explore various control concepts, such as open-loop deadbeat control~\cite{wu20133,palmer2014periodic} or reachability~\cite{piovan2015reachability}. \par
% While many efforts have been made to map these directly to the full-order model~\cite{wensing2013high,hutter2010slip,poulakakis2009spring}, typically this step requires substantial effort in terms of implementation and computation. The resulting control is typically brittle to model inaccuracies. Furthermore, forcing the high-dimensional dynamics of the actual system to behave on the low-dimensional manifold of the SLIP can potentially override the true natural dynamics. For these reasons, we recommend find the value of these models in their \emph{descriptive} role. We do not recommend their \emph{prescriptive} use to design formal controllers. \par
% Most of my work is built around the analysis of the SLIP model, since it can be reduced to a 1-dimensional state-space and 1-dimensional action-space. I would like to emphasize that we choose to use this as a \emph{descriptive} model, that is, for analysis. I do not recommend directly using it as a prescriptive model, that is, to design controllers assuming the system will behave exactly like a SLIP model. \par

\section{Learning control}

The recent success in machine learning, in particular with reinforcement learning (RL), provides another alternative to model-based optimal control. My phd work has been heavily influenced by ideas from reinforcement learning, especially the notion of state-action space. For readers less familiar with this field, I will introduce the essential topics needed to understand and appreciate this thesis.

% \subsection{Notes on terminology}
% Before beginning, 

\subsection{Reinforcement learning is model-free optimal control}
At its core, reinforcement learning (RL) is based on Bellman updates, and is using the same key insight as dynamic programming and optimal control: the \emph{principle of optimality}.
%In a simple discrete-time case, an update takes the form of
% \begin{equation}\label{eq:bellman}
% V(s) = r(s, a) + V(s')
% \end{equation}
% where $V$ is the value function, $r$ is the received when at state $s$ and taking action $a$, and $s'$ denotes the state at the next time-step.
% In layman terms, the principle of optimality states that, given a known optimal path from $A$ to $B$, any portion of that path is also optimal for the starting and end-points of that portion. The insight this provides is that the optimal control problem can be split into subproblems to be solved individually.
% Retruning to the Bellman updates, this allows us to consider only the immediate next step
For the interested reader, I recommend comparing two formulations of value iteration: the one introduced in chapter 4.4 of the classical RL textbook by~\textcite{sutton2018book}, and the one introduced in chapter 5.3 of the classical optimal control textbook by~\textcite{bertsekas2017book}. Aside from notation and a minor detail\footnote{In~\cite{sutton2018book}, there is also a discount factor which is introduced a few pages after the initial formulation in~\cite{bertsekas2017book}.}, the equations are the same and will converge for the same reason. The critical difference is the presence (or absence) of a model, which dictates how updates can be applied.
In the model-based case, the transitions and their associated costs are known, and each iteration can make a batch update for every state. The most straightforward course is to back out updates from the goal state. In the model-free case, the cost of a specific transition can only be discovered through experience. \par
%To gather this experience, we roll out one episode, applying a policy at each step to choose the next action. Policy iteration (\cite[cf. chapter 4.3]{sutton2018book}) considers a fixed policy, and finishes with the value function of that policy. 
Since the cost associated with a transition cannot be known a priori by consulting a model, it is convenient to keep track of this cost by associating it to a given state-action pair, instead of just the state. Reasoning on not just the cost, but the value of a state-action pair leads to one of the breakthroughs in RL: $q$-learning~\cite[cf. chapter 6.5]{sutton2018book}. A major advantage of this approach is that the optimal value function is not specific to a given optimal policy: if the optimal policy is not unique, all optimal policies result in the same optimal value function. We will use this idea in~\cite{heim2019beyond} to quantify the robustness of a system without pre-specifying a policy.

\subsection{The challenge of robot learning}

\textcite{kober2013reinforcement} have written a comprehensive overview of RL for robotics. Although slightly dated, the primary challenges (\cite[see 8.1 in]{kober2013reinforcement}) have not changed considerably. Many core challenges stem from the difficulty of obtaining samples from real-world experiments. \par
Perhaps the most obvious difficulty is that, unlike in simulations, hardware experiments can only run at physical time, and are difficult to parallelize. Although there have been some efforts to learn control policies with a large number of robots~\cite{levine2018learning}, these tend to be very limited in scale (14 robots in~\cite{levine2018learning}), and do not provide the amount of data conventional machine learning approaches are designed for.
This is largely because actual hardware experiments tend to require large amounts of space and funding; unlike for simulations, the cost per experiment does not decrease dramatically with scale.
A more fundamental bottleneck is user time: a user typically needs to set up each run, reset it after failures, and maintain and repair the robots. \par
Models can help in two ways: by allowing learning in simulation, and by informing the structure of the control policy.
 \par
Models allow simulations to be used to bootstrap the learning process. Collecting data in simulation is cheap, and the policy can then be refined on the real robot. However, the transfer from simulation to the real-world is not always straight-forward.
The models used in simulation are often insufficiently accurate, especially for legged locomotion~\cite{neunert2017off,tan2018sim}, and the learned policies often over-fit to these models. In other words, they find policies that may be optimal (or at least work reasonably well) but are utterly not robust. This is commonly known as the \emph{sim-to-real} problem. \par
A solution to this is dynamics randomization~\cite{peng2017sim,lowrey2018reinforcement,tan2018sim}: the parameters of the simulation are perturbed in each learning iteration, with the hope that the resulting control policy is robust to differences in the model. While this approach shows promise, to the best of my knowledge there is no principled approach to choose which parameters to perturb, or by how much. \par
Another approach is to ensure your simulation is accurate:~\textcite{hwangbo2019learning} train a deep neural network to simulate the motor dynamics, which are typically difficult to model with first principles. This is then used to train a policy in simulation which can be transferred to the robot\footnote{Dynamics randomization is also used in this study, but the core novelty is the use of supervised learning to obtain an accurate simulation.}. \par
In all of these approaches, one of the most critical feedback loops is the designer, typically a graduate student.~\textcite{xie2019cassie} describe in substantial detail the iterations involved in training a policy in simulation, which they eventually deploy successfully to the Cassie biped, without any dynamics randomization. \par
Each of these approaches addresses the same fundamental issue: the need for robustness. \par
%Often, these simulation-based policies would cause critical damage to the robot and surroundings if transferred directly. 
Model-based control can also be used directly for controller design, effectively warm-starting the learning process. In some of the most straight-forward approaches, tools from conventional control theory and learning control are combined but kept distinct. For example,~\textcite{kumar2018improving} design an initial stabilizing LQR-based balance and stepping controller based on a linear inverted pendulum model. This is overlayed with a deep neural network which outputs offsets to the joint torques and target angles.
In another example, ~\textcite{yeganegi2019robust} start with a standard hierarchical control setup: a trajectory optimization problem is formulated using a linear inverted pendulum model. This generates CoM trajectories, which are then tracked with a whole-body controller. However, how robustly these trajectories can be tracked depends largely on the coefficients of the cost-function used in the high-level controller. A Bayesian optimization scheme is then used to search for cost coefficients which result in trajectories that are both robust and fast. \par
% Lin + Ponton would be another good example
% Also heijmink
Conventional control theory can also inform the design of proper policy parameterization. An appropriately chosen parameterization has significant effect on the size of the search space as well as the resulting stability of the closed-loop system~\cite{roberts2011feedback}. Indeed, directly learning a policy which outputs motor torques is often ineffective, especially for systems that are dynamically unstable~\cite{peng2017learning}. In recent work,~\textcite{viereck2018learning} design a policy which outputs a combination of desired trajectories and feedback gains to stabilize these trajectories. This allows the agent to reason at a higher level, and results in a more robust and reliable outcome than directly learning a state-to-torque mapping~\cite{viereck2018learning}.
% It is also useful to incorporate the structure of the control policy gleaned from models directly into the search space of the learning agent. This can reduce the size of the search space, and also allow more accurate extrapolation from previously gathered data. For example,
In another example,~\textcite{marco2017design} incorporate the structure of a linear-quadratic regulator (LQR) into the kernel of a Gaussian process (GP). This also allows the policy to extrapolate accurately farther away from previously observed samples. \par
% ~\textcite{theodorou2010reinforcement} formulated the policy improvement by path integrals (PI\textsuperscript{2}), which directly leverages the structure of the optimal controller for a certain class of problems. The conditions of interest to us arre that the costs and dynamics should be quadratic and affine\footnote{Rigid-body systems are always affine with respect to torques and forces.}, respectively, with respect to the control inputs. For these systems, the Hamilton-Jacobi-Bellman equation results in a partial differential equation which can be represented as a stochastic differential equation describing the evolution of the probability distribution of 
% Read and include PI2

% Models are also useful, as they can inform on the structure of the policy. They tell us a lot of which space/representation the learning algorithm should operate in. This can greatly reduce the search space. \par
% \textbf{Shaping}
% Designing an appropriately rich reward function, also called \emph{shaping}, is not a necessary evil, but rather a powerful tool. Though excessive massaging of a reward function can heavily bias the learning outcome, it also helps convergence tremendously. Two main approaches to temporarily provide a more informative reward are imitation learning\footnote{We do not refer to pure imitation learning, but rather mixing imitation learning with learning with experience. This is sometimes referred to as apprenticeship learning in older literature, however the field as a whole generally uses the term imitation learning in the broader sense, so we will remain consistent with this.} and curriculum learning. \par
% In imitation learning, the learning process is bootstrapped with a few expert demonstrations. In the first phase, the learning agent simply tries to find a policy which minimizes the error between it's behavior and the demonstrations. 

% Imitation learning is blah blah. Advantages and pitfalls. It's challenges are the assumption that the demonstrator is optimal. This is the case for model-based optimal control GPS, Viereck. This can also be used to infer safety, Glen Chou. Recent work is starting to incorporate notions of sub-optimal demonstrators, which is exciting Anca Dragan. \par
% Curriculum learning is blah blah. It the main drawback is that it takes a lot of intuition to design effective and general curricula. There is interest in automating curriculum design INRIA/PETERS. There is also work leveraging backreachability for designing curricula BaRC.
One of the early promises of reinforcement learning is that very sparse rewards could be given based on task-completion. In recent years, the community has come to realize that designing an appropriately rich reward function is not a necessary evil, but rather a powerful tool. Though excessive massaging of a reward function can heavily bias the learning outcome, it also helps convergence tremendously by \emph{shaping} the reward landscape. In effect, the purpose of this proxy reward function is to allow the learning agent to sample gradient information more reliably. An effective compromise is to use this proxy only temporarily: once the the learning agent has converged to a reasonable starting policy, it can continue learning without the assistance of the more informative reward function. This is called \emph{curriculum learning}~\cite{bengio2009curriculum,karpathy2012curriculum}. \par
While there is empirical evidence that this approach can be very effective, it also requires a lot of intuition and experience to design effective and general curricula. This may require a lot of insight of both the system and the task at hand, partially defeating the purpose of learning the control policy instead of designing it. There is great interest recently in automating the design of curricula, either by modeling student progress as a separate optimization problem~\cite{portelas2019teacher}, or allowing the student itself some control over the task difficulty~\cite{klink2019self}. The work of~\textcite{kumar2018improving}, mentioned above, also incorporates an adaptive curriculum  by applying perturbations close to the edge of the currently known region of attraction. By collecting samples near the edge of stability, these samples are very likely to be both highly informative and remain safe.~\textcite{ivanovic2019barc} formally combine concepts from curriculum learning and backreachability to effectively learn from a sparse reward signal. In this case, the learning agent begins training near the target goal, such that even a random walk is likely to sample the reward. The agent is initialized from a successively larger set of initial states. Instead of increasing this set arbitrarily, it is increased by computing an approximate backwards reachable set over a short time horizon. In this manner, the curriculum directly takes the dynamics of the system into consideration.