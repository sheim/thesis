% !TEX root = ../root.tex

\chapter{Preliminaries}
We introduce here relevant background in a concise manner.

\section{Dynamics and Control for Legged Locomotion}
Dynamics of legged locomotion include many interesting properties that make them particularly challenging to control. They are non-smooth, due to impacts at foot touchdown. They are hybrid, as the governing equations of motion switch abruptly every time a foot touches down or lifts off. They are underactuated, due to the floating base. And of course they are typically also nonlinear. For most systems of interest to us, they are also passively unstable, and require feedback control. \par

A recurrent theme in understanding legged locomotion is to \emph{exploit the natural dynamics} of the system. Indeed, despite the very rich dynamics, it is possible to achieve locomotion with very simple approaches that exploit the natural dynamics.
Examples include passive dynamic walkers McGEER and actuated robots based on them RANGER/TEDRAKE/WISSE.
Also, controllers based on oscillators AUKE, often inspired by central pattern generators as studied in biology MATSUOKA. These can even produce diverse gaits without explicit planning ISHIGURO.
However, these approaches are often limited in performance and versatility, and rely a lot on intuition and trial-and-error. \par

Optimal control has become the preeminent tool of the field, for clear reasons.  MATH. Optimal control directly embeds the structure of the system dynamics into the controller, in the form of constraints. The designer is then left substantial freedom to design the overall behavior in the cost function. As a result, optimal control is used in all aspects of analysis, not only control. For co-design, an optimal controller is assumed, and black-box optimization is performed for the mechanical structure REMY, COROS. In biology, a lot is explained by means of assuming the cost function is somehow optimal. Usually energy KUO/DONELAN/TAYLORHOYT. \par

\subsection{Reduced-order Models of Running}

A major challenge is still scalability. In high-dimensional systems, optimal control is often limited to finding locally optimal solutions. Alternatively, a popular approach is to split control into hierarchies. A high-level controller reasons on a reduced-order model, which becomes tractable. These solutions are then mapped to the full state-space with RIGHETTI/TEDRAKE/WENSING. These reduced-order models are also particularly convenient for analysis: it is easier for us to reason about low-dimensional systems that we can easily visualize. Furthermore, they can amenable to more powerful but less tractable tools, including brute-force. \par

It is important at this point to make a distinction between models that are useful for control and models that are useful for analysis. For control, a reduced-order model should be easy to map back into the full-order model, and provide provable guarantees. For example, the LIP assumes a constant height for walking, which does not describe natural walking well. It does however make computation tractable (convex?) and can provide guarantees, making it a good choice for control. A model for analysis should be easy to understand, and closely match the qualitative properties of interest. For example, the spring-loaded inverted pendulum (SLIP) provides very good intuitive understanding for the compliant properties of legs, both in running animals BLICKHAN/GEYER/JINDRICH, as well as in some robots RAIBERT/RHEX. They have also been used to conceptually explore various control concepts, such as open-loop deadbeat control GEYER/PALMER or reachability BYL. While some efforts have been made to map these directly to the full-order model WENSING/HUTTER, typically this step requires substantial effort (both in terms of implementation and computation), can potentially override the true natural dynamics and thus be inefficient, and are brittle to model inaccuracies. Most of my work is built around the analysis of the SLIP model, since it can be reduced to a 1-dimensional state-space and 1-dimensional action-space. I would like to emphasize that we choose to use this as a \emph{descriptive} model, that is, for analysis. I do not recommend directly using it as a prescriptive model, that is, to design controllers assuming the system will behave exactly like a SLIP model.

Also bring in some biology uses, such as DALEY/HUBICKI/HAEUFLE.

\section{Learning Control}

The recent success in machine learning, in particular with reinforcement, is pushing another alternative to conventional model-based optimal control. The work of my phd has been heavily influenced by ideas from reinforcement learning, especially the concept of state-action space and model-free control. For readers less familiar with this field, I will give a brief introduction of the concepts that are useful for understanding my work.

\subsection{Model-free and model-based}
Let us start with standard reinforcement learning. At its core, it is simply a model-free version of optimal control. EXAMPLE WITH VALUE ITERATION AND Q-LEARNING.
At the end of the day, all just Bellman updates. \par

While early efforts in reinforcement learning emphasized the strength of being purely model-free, it has since become clear that models are indeed useful. Although model-predictions are never perfectly accurate, they are usually still quite good, especially for the systems we are interested in. A lot of more recent effort in learning control mixes model-free and model-based tools. \par

Learn a model, then use it (sys ID).

Start with model-based tools, then learn an additional additive controller to account for un-modeled dynamics.

Use a model to generate data (simulation).

\subsection{Shaping}

In RL, shaping refers to shaping the reward-landscape, in order to make it easier to for a learning agent to learn on.
Most of the effort in this field is called curriculum design: the agent learns on easier tasks which inform it about the original policy.
There can also be shaping by modifying the reward function. This can be a temporary, handcrafted change, or it can be IRL.
It is akin to solving a convex approximation of the cost-function in model-based optimization. As RL often takes a model-free approach, usually there is no emphasis on lower and upper bounds (as there is in model-based optimization).
Mostly focused on changing $R$ in the MDP, in some cases a combination of $R$ and $P$.

% \section{Template Models}
% To explore 

\section{Viability and Backreachability}

The core of my work is based on viability theory, first pioneered by~\citet{aubin2011viability}. This is an awesome way to think about things!

Definition of viability. Connection to backreachable sets. \par

Example applications. \par

Challenges (scaling). \par