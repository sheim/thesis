% !TEX root = ../root.tex

\chapter{Preliminaries} \label{chap:prelims}
This chapter introduces relevant background information, to help readers from potentially different backgrounds to quickly gain enough knowledge to understand the context, relevance and importance of the contributions of each paper. Covered are the basics of legged locomotion, reinforcement learning and viability theory.

\section{Legged Locomotion}

While most results in my publications are valid for dynamical systems in general, my research has always been grounded in the topic of legged locomotion. This field offers abundant natural inspiration, including ourselves. Legged systems also encapsulate many interesting challenges in dynamics: the dynamics of legged systems are non-smooth, due to impacts at foot touchdown. They are hybrid, as the governing equations of motion switch abruptly every time a foot touches down or lifts off. They are underactuated, due to the floating base. And of course they are typically also highly nonlinear. While passively stable legged systems exist, for most systems of interest to us they are passively unstable, and require feedback control.
On top of all this, the community of dynamic legged locomotion is both vibrant and friendly.

\subsection{Dynamics and Control}
Dynamics of legged locomotion include many interesting properties that make them particularly challenging to control.  \par

A recurrent theme in understanding legged locomotion is to \emph{exploit the natural dynamics} of the system. Indeed, despite the very rich dynamics, it is possible to achieve locomotion with very simple approaches that exploit these natural dynamics.
The most extreme examples include passive dynamic walkers~\cite{mcgeer1990passive} and actuated robots based on them~\cite{bhounsule2012design,wisse2006design,tedrake2005learning}. \par
Making a small step in the direction of control are legged robots governed by clocks and oscillators, such as those developed by~\textcite{sprowitz2013towards,buchli2006resonance,altendorfer2001rhex,owaki2013simple}. However, these approaches are often limited in performance and versatility, and typically rely on a lot of intuition and trial-and-error. \par
Optimal control offers a more explicit approach, in the form of 
\begin{align}
& \min_{u} J(x, u, t) \\
& \text{subject to } \phi(u, x, t) \leq 0,
\end{align}
where $u$ is the control input, $x$ is the state of the system, $t$ is time, $J$ is an arbitrary, scalar cost function and $\phi$ is a vector of constraint functions. The structure of the dynamics can be directly incorporated into the control law in the form of constraints, such that the control law directly and explicitly considers the natural dynamics of the system. Furthermore, most robots of interest are well described as rigid-body systems, and powerful and mature tools are readily available for modeling, identifying, synthesizing and computing controllers for these system. A designer is then allowed substantial freedom to design the overall behavior by modifying the cost function. As a result, optimal control has become one of the preeminent tools of the field, not only for control and planning~\cite{koolen2016design,ponton2016convex,winkler2018gait,mombaur_2009,deits2014footstep}, but also analysis and design~\cite{ha2018codesign,takahashi2019spring,mombaur_2009,Yesilevskiy_2018,Birn-Jeffery3786}.


Also bring in some biology uses, such as DALEY/HUBICKI/HAEUFLE.


\section{Learning Control}

The recent success in machine learning, in particular with reinforcement learning (RL), is pushing another alternative to conventional model-based optimal control. The work of my phd has been heavily influenced by ideas from reinforcement learning, especially the concept of state-action space and model-free control. For readers less familiar with this field, I will give a brief introduction of the concepts that are useful for understanding my work.

\subsection{Model-free and model-based}
At its core, reinforcement learning is simply model-free optimal control. Just as dynamic programming and value iteration, the standard RL algorithm is based on Bellman updates. 

Let us start with standard reinforcement learning. At its core, it is simply a model-free version of optimal control. EXAMPLE WITH VALUE ITERATION AND Q-LEARNING.
At the end of the day, all just Bellman updates. \par

While early efforts in reinforcement learning emphasized the strength of being purely model-free, it has since become clear that models are indeed useful. Although model-predictions are never perfectly accurate, they are usually still quite good, especially for the systems we are interested in. A lot of more recent effort in learning control mixes model-free and model-based tools. \par

Learn a model, then use it (sys ID).

Start with model-based tools, then learn an additional additive controller to account for un-modeled dynamics.

Use a model to generate data (simulation).

\subsection{Shaping}

In RL, shaping refers to shaping the reward-landscape, in order to make it easier to for a learning agent to learn on.
Most of the effort in this field is called curriculum design: the agent learns on easier tasks which inform it about the original policy.
There can also be shaping by modifying the reward function. This can be a temporary, handcrafted change, or it can be IRL.
It is akin to solving a convex approximation of the cost-function in model-based optimization. As RL often takes a model-free approach, usually there is no emphasis on lower and upper bounds (as there is in model-based optimization).
Mostly focused on changing $R$ in the MDP, in some cases a combination of $R$ and $P$.

% \section{Template Models}
% To explore 

\section{Viability and Backreachability}

The core of my work is based on viability theory, first pioneered by~\textcite{aubin2011viability}. The development of viability theory is largely motivated by observations of dynamical systems in nature and society, whose behavior somehow avoids chaos or pure randomness, yet never seem to settle at a resting state, an equilibrium.
Simple examples are Darwinian evolution, economics or politics of state. There is no terminal equilibrium state for these systems, or at least, none that we can identify or foresee.
The classical mathematical tools based on convergence to such an equilibrium state are therefore ill-suited to describe them, although many have found ways to adapt them to the task, for example by assuming a time-varying equilibrium state, which the system chases but never reaches. \par
Viability theory provides are more appropriate and direct description. First, sets of failure states are defined, which the system must be able to avoid. From this naturally emerges the \emph{viability kernel}, the maximal set of states from which there exist control inputs which keep the system inside the viability kernel, under the constraint of never entering the set of failure states. In simpler terms, if the system ever leaves the viability kernel, it means it can no longer return inside of it, and is doomed to eventually enter the failure set.
The viability kernel thus includes all possible regions (or "basins") of attraction, since these regions avoid the failure set by their quality of converging to a non-failing state. However, it does not require the property of convergence convergence, and the system is free to roam the viability kernel, even stochastically, as long as it never leaves this set. \par
This provides us with a powerful way of thinking, which is particularly appropriate for considering learning systems. Indeed, although the final goal in learning control may be a control policy which generates a large region of attraction with strong convergence properties, these properties typically cannot be relied on during the learning process. \par
Viability theory therefore allows us to make very general statements of a dynamical system's behavior. However, this comes at a price: computing viable sets is typically computationally expensive, and for the types of dynamics we are most interested in it often relies on brute force. A related approach is the computation of back-reachable sets. The back-reachable set is the set of all states from which the system can reach a specified target set. This differs from convergence to equilbria, for two important distinctions. First, it does not assume convergence towards the target set, only the existence of convergence. Second, the target set does not necessarily include equilibria, and the system may not necessarily be able to remain inside the target set after reaching it. In practice, viability kernels and the back-reachable sets are largely interchangeable. For many cases of interest, they happen to coincide. Indeed, the viability kernel is in essence the back-reachable set of itself. For an introduction to methods for computing these sets, we recommend \cite{bansal2017hamilton,liniger2017real}.